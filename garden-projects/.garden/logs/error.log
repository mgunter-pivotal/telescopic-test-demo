
[2021-04-02T10:09:53.918Z] Failed resolving one or more modules:

udp-app: Module udp-app neither specifies image nor provides Dockerfile

Error Details:

errors: {}


[2021-04-02T10:34:54.359Z] Failed resolving one or more modules:

udp-test: Error validating Module 'udp-test': key "type" is not allowed at path .services[0][ports][0][type]

Error Details:

errors: {}


[2021-04-02T10:35:07.077Z] Failed resolving one or more modules:

udp-test: Module udp-test neither specifies image nor provides Dockerfile

Error Details:

errors: {}


[2021-04-02T10:41:43.575Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=recorder --output=json -f -" failed with code 1:

The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid value: 30000: provided port is already allocated

The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid value: 30000: provided port is already allocated

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30000: provided port is already allocated
all: >-
  The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30000: provided port is already allocated
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-02T10:41:43.596Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

Error from server (Invalid): error when creating "STDIN": Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error from server (Invalid): error when creating "STDIN": Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (Invalid): error when creating "STDIN": Service
  "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001:
  provided port is already allocated

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
all: >-
  Error from server (Invalid): error when creating "STDIN": Service
  "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001:
  provided port is already allocated

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-02T10:42:51.509Z] Error deploying backend: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment backend: ScalingReplicaSet - Scaled up replica set backend-7874b6588f to 1
Pod backend-7874b6588f-pqlgv: Scheduled - Successfully assigned suitcaselab-default/backend-7874b6588f-pqlgv to docker-desktop
Pod backend-7874b6588f-pqlgv: Pulled - Container image "udp-test:v-bb3fcb6989" already present on machine
Pod backend-7874b6588f-pqlgv: Created - Created container backend
Pod backend-7874b6588f-pqlgv: Failed - Error: failed to start container "backend": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: exec: "python3 road-surface-detect.py": executable file not found in $PATH: unknown
Pod backend-7874b6588f-pqlgv: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/backend

****** backend-7874b6588f-pqlgv ******
------ backend ------<no logs>

Error Details:

serviceName: backend
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment backend:\e[39m \e[37mScalingReplicaSet - Scaled up replica set backend-7874b6588f to 1\e[39m\n\e[94mPod backend-7874b6588f-pqlgv:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/backend-7874b6588f-pqlgv to docker-desktop\e[39m\n\e[94mPod backend-7874b6588f-pqlgv:\e[39m \e[37mPulled - Container image \"udp-test:v-bb3fcb6989\" already present on machine\e[39m\n\e[94mPod backend-7874b6588f-pqlgv:\e[39m \e[37mCreated - Created container backend\e[39m\n\e[94mPod backend-7874b6588f-pqlgv:\e[39m \e[33mFailed - Error: failed to start container \"backend\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"python3 road-surface-detect.py\": executable file not found in $PATH: unknown\e[39m\n\e[94mPod backend-7874b6588f-pqlgv:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/backend\e[39m\n\e[94m\e[39m\n\e[94m****** backend-7874b6588f-pqlgv ******\e[39m\n\e[94m\e[39m\e[90m------ backend ------\e[39m<no logs>"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: backend
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/backend
      uid: f886c9ab-b962-49b4-93ce-a98f41d73bf1
      resourceVersion: '410176'
      generation: 1
      creationTimestamp: '2021-04-02T10:42:19Z'
      labels:
        module: udp-test
        service: backend
      annotations:
        deployment.kubernetes.io/revision: '1'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 3018128637594b799b9462e9c66ed1e9cd36739f16be903fa9a2300c428ff7eb
        garden.io/version: v-bb3fcb6989
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"3018128637594b799b9462e9c66ed1e9cd36739f16be903fa9a2300c428ff7eb","garden.io/version":"v-bb3fcb6989"},"labels":{"module":"udp-test","service":"backend"},"name":"backend","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"backend"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"backend"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-bb3fcb6989"},{"name":"GARDEN_MODULE_VERSION","value":"v-bb3fcb6989"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-bb3fcb6989\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-bb3fcb6989","imagePullPolicy":"IfNotPresent","name":"backend","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: backend
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: backend
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 1
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T10:42:51.520Z] 3 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-02T10:41:38.675Z
    completedAt: 2021-04-02T10:41:43.589Z
    batchId: 269a0d64-e618-4b6f-9765-c0f83b497773
    version: v-78a2fad047
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-02T10:41:38.668Z
    completedAt: 2021-04-02T10:41:43.568Z
    batchId: cece05b8-f79c-4262-9da7-7364d6f0abaa
    version: v-12082550ca
  deploy.backend:
    type: deploy
    description: deploying service 'backend' (from module 'udp-test')
    key: deploy.backend
    name: backend
    startedAt: 2021-04-02T10:42:19.066Z
    completedAt: 2021-04-02T10:42:51.499Z
    batchId: b9a90baa-dffb-4004-bd5d-8ebc5440685c
    version: v-bb3fcb6989


[2021-04-02T10:53:07.619Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=recorder --output=json -f -" failed with code 1:

The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid value: 30000: provided port is already allocated

The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid value: 30000: provided port is already allocated

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30000: provided port is already allocated
all: >-
  The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30000: provided port is already allocated
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-02T10:53:07.634Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
all: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-02T10:53:11.027Z] 2 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-02T10:53:02.510Z
    completedAt: 2021-04-02T10:53:07.628Z
    batchId: 2324e28d-fb01-4b99-9908-52c368f35ac8
    version: v-78a2fad047
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-02T10:53:02.497Z
    completedAt: 2021-04-02T10:53:07.609Z
    batchId: 5571f8c0-77a2-482e-91f6-7e5cf0084abf
    version: v-12082550ca


[2021-04-02T10:59:39.132Z] Timed out waiting for recorder to deploy

Error Details:

statuses: []


[2021-04-02T10:59:39.189Z] Timed out waiting for player to deploy

Error Details:

statuses: []


[2021-04-02T10:59:39.207Z] 2 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-02T10:54:24.924Z
    completedAt: 2021-04-02T10:59:39.185Z
    batchId: af6c9b4e-93e0-45f7-90cc-53ed855900b5
    version: v-78a2fad047
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-02T10:54:24.913Z
    completedAt: 2021-04-02T10:59:39.122Z
    batchId: 52d5eef0-4833-444a-b700-cdc954be7ed9
    version: v-12082550ca


[2021-04-02T11:03:02.655Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Pod inbound-traffic-d484d7dd5-76c8b: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-d484d7dd5-76c8b to docker-desktop
Pod inbound-traffic-d484d7dd5-76c8b: Pulled - Container image "udp-test:v-636bb2f0a9" already present on machine
Pod inbound-traffic-d484d7dd5-76c8b: Created - Created container inbound-traffic
Pod inbound-traffic-d484d7dd5-76c8b: Failed - Error: failed to start container "inbound-traffic": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: exec: "python": executable file not found in $PATH: unknown
Pod inbound-traffic-d484d7dd5-76c8b: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-d484d7dd5-76c8b ******
------ inbound-traffic ------<no logs>

Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mPod inbound-traffic-d484d7dd5-76c8b:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-d484d7dd5-76c8b to docker-desktop\e[39m\n\e[94mPod inbound-traffic-d484d7dd5-76c8b:\e[39m \e[37mPulled - Container image \"udp-test:v-636bb2f0a9\" already present on machine\e[39m\n\e[94mPod inbound-traffic-d484d7dd5-76c8b:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-d484d7dd5-76c8b:\e[39m \e[33mFailed - Error: failed to start container \"inbound-traffic\": Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown\e[39m\n\e[94mPod inbound-traffic-d484d7dd5-76c8b:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-d484d7dd5-76c8b ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39m<no logs>"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '412763'
      generation: 1
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '1'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 3df7f0332f1a61cb5e95bf971c38c83b00bc24361633acd105ac0ea70b96d594
        garden.io/version: v-636bb2f0a9
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"3df7f0332f1a61cb5e95bf971c38c83b00bc24361633acd105ac0ea70b96d594","garden.io/version":"v-636bb2f0a9"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-636bb2f0a9"},{"name":"GARDEN_MODULE_VERSION","value":"v-636bb2f0a9"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-636bb2f0a9\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-636bb2f0a9","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 1
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:03:05.899Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:02:58.796Z
    completedAt: 2021-04-02T11:03:02.647Z
    batchId: 4aebc551-f616-4140-94c8-cc635e86db87
    version: v-636bb2f0a9


[2021-04-02T11:04:16.213Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Pod inbound-traffic-575db9885f-cjwkd: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-575db9885f-cjwkd to docker-desktop
Pod inbound-traffic-575db9885f-cjwkd: Pulled - Container image "udp-test:v-3702db6921" already present on machine
Pod inbound-traffic-575db9885f-cjwkd: Created - Created container inbound-traffic
Pod inbound-traffic-575db9885f-cjwkd: Started - Started container inbound-traffic
Pod inbound-traffic-575db9885f-cjwkd: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-575db9885f-cjwkd ******
------ inbound-traffic ------<no logs>

Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mPod inbound-traffic-575db9885f-cjwkd:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-575db9885f-cjwkd to docker-desktop\e[39m\n\e[94mPod inbound-traffic-575db9885f-cjwkd:\e[39m \e[37mPulled - Container image \"udp-test:v-3702db6921\" already present on machine\e[39m\n\e[94mPod inbound-traffic-575db9885f-cjwkd:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-575db9885f-cjwkd:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-575db9885f-cjwkd:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-575db9885f-cjwkd ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39m<no logs>"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '412973'
      generation: 2
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '2'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: d09256237c324d17a575f7afc51e7b011aac37e32b84ba83d0f40181d6ae5274
        garden.io/version: v-3702db6921
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"d09256237c324d17a575f7afc51e7b011aac37e32b84ba83d0f40181d6ae5274","garden.io/version":"v-3702db6921"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-3702db6921"},{"name":"GARDEN_MODULE_VERSION","value":"v-3702db6921"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-3702db6921\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-3702db6921","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 2
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:04:16.238Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:04:09.932Z
    completedAt: 2021-04-02T11:04:16.205Z
    batchId: 6c558f59-3380-4bed-b49d-a98dad0ac55d
    version: v-3702db6921


[2021-04-02T11:06:35.334Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0
Pod inbound-traffic-d48dcfd55-qq2gq: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-d48dcfd55-qq2gq to docker-desktop
Pod inbound-traffic-d48dcfd55-qq2gq: Pulled - Container image "udp-test:v-34de9cb875" already present on machine
Pod inbound-traffic-d48dcfd55-qq2gq: Created - Created container inbound-traffic
Pod inbound-traffic-d48dcfd55-qq2gq: Started - Started container inbound-traffic
Pod inbound-traffic-d48dcfd55-qq2gq: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-d48dcfd55-qq2gq ******
------ inbound-traffic ------Error opening terminal: unknown.


Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0\e[39m\n\e[94mPod inbound-traffic-d48dcfd55-qq2gq:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-d48dcfd55-qq2gq to docker-desktop\e[39m\n\e[94mPod inbound-traffic-d48dcfd55-qq2gq:\e[39m \e[37mPulled - Container image \"udp-test:v-34de9cb875\" already present on machine\e[39m\n\e[94mPod inbound-traffic-d48dcfd55-qq2gq:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-d48dcfd55-qq2gq:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-d48dcfd55-qq2gq:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-d48dcfd55-qq2gq ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39mError opening terminal: unknown.\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '413313'
      generation: 3
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '3'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 3f0016a03e16aa27e9f8dadc9186977cb09c8d9c0fa9d2665d816c303cb43c79
        garden.io/version: v-34de9cb875
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"3f0016a03e16aa27e9f8dadc9186977cb09c8d9c0fa9d2665d816c303cb43c79","garden.io/version":"v-34de9cb875"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-34de9cb875"},{"name":"GARDEN_MODULE_VERSION","value":"v-34de9cb875"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-34de9cb875\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-34de9cb875","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 3
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:06:35.354Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:06:31.478Z
    completedAt: 2021-04-02T11:06:35.326Z
    batchId: 01a6f67f-6f26-4f3d-887e-6946d5a3089c
    version: v-34de9cb875


[2021-04-02T11:07:33.153Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0
Pod inbound-traffic-6484d44f6f-6gspl: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-6484d44f6f-6gspl to docker-desktop
Pod inbound-traffic-6484d44f6f-6gspl: Pulled - Container image "udp-test:v-c01a18123c" already present on machine
Pod inbound-traffic-6484d44f6f-6gspl: Created - Created container inbound-traffic
Pod inbound-traffic-6484d44f6f-6gspl: Started - Started container inbound-traffic
Pod inbound-traffic-6484d44f6f-6gspl: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-6484d44f6f-6gspl ******
------ inbound-traffic ------Dockerfile
Requirements.txt
road-surface-detect.py


Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0\e[39m\n\e[94mPod inbound-traffic-6484d44f6f-6gspl:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-6484d44f6f-6gspl to docker-desktop\e[39m\n\e[94mPod inbound-traffic-6484d44f6f-6gspl:\e[39m \e[37mPulled - Container image \"udp-test:v-c01a18123c\" already present on machine\e[39m\n\e[94mPod inbound-traffic-6484d44f6f-6gspl:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-6484d44f6f-6gspl:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-6484d44f6f-6gspl:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-6484d44f6f-6gspl ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39mDockerfile\nRequirements.txt\nroad-surface-detect.py\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '413491'
      generation: 4
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '4'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 991ac7ab168c89be0970d2719cfbc335625b1841bdee5526f73f255434b91850
        garden.io/version: v-c01a18123c
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"991ac7ab168c89be0970d2719cfbc335625b1841bdee5526f73f255434b91850","garden.io/version":"v-c01a18123c"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-c01a18123c"},{"name":"GARDEN_MODULE_VERSION","value":"v-c01a18123c"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-c01a18123c\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-c01a18123c","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 4
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:07:33.180Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:07:29.289Z
    completedAt: 2021-04-02T11:07:33.144Z
    batchId: baa1468a-f0ec-419d-b9d4-52bed4105245
    version: v-c01a18123c


[2021-04-02T11:07:56.741Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0
Pod inbound-traffic-645cdc9f89-xc9rh: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-645cdc9f89-xc9rh to docker-desktop
Pod inbound-traffic-645cdc9f89-xc9rh: Pulled - Container image "udp-test:v-0719ae3cf4" already present on machine
Pod inbound-traffic-645cdc9f89-xc9rh: Created - Created container inbound-traffic
Pod inbound-traffic-645cdc9f89-xc9rh: Started - Started container inbound-traffic
Pod inbound-traffic-645cdc9f89-xc9rh: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-645cdc9f89-xc9rh ******
------ inbound-traffic ------/app


Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0\e[39m\n\e[94mPod inbound-traffic-645cdc9f89-xc9rh:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-645cdc9f89-xc9rh to docker-desktop\e[39m\n\e[94mPod inbound-traffic-645cdc9f89-xc9rh:\e[39m \e[37mPulled - Container image \"udp-test:v-0719ae3cf4\" already present on machine\e[39m\n\e[94mPod inbound-traffic-645cdc9f89-xc9rh:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-645cdc9f89-xc9rh:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-645cdc9f89-xc9rh:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-645cdc9f89-xc9rh ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39m/app\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '413583'
      generation: 5
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '5'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 9bff67afb4ad8547a35644ee190c048c0d208c80d9b1ac632a25b2aa870032f6
        garden.io/version: v-0719ae3cf4
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"9bff67afb4ad8547a35644ee190c048c0d208c80d9b1ac632a25b2aa870032f6","garden.io/version":"v-0719ae3cf4"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-0719ae3cf4"},{"name":"GARDEN_MODULE_VERSION","value":"v-0719ae3cf4"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-0719ae3cf4\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-0719ae3cf4","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 5
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:07:56.765Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:07:52.848Z
    completedAt: 2021-04-02T11:07:56.731Z
    batchId: 46045911-7e28-4a1e-8496-02716d356cf1
    version: v-0719ae3cf4


[2021-04-02T11:08:13.318Z] Error deploying inbound-traffic: CrashLoopBackOff - back-off 10s restarting failed container=inbound-traffic pod=inbound-traffic-ccf46495f-dqnv2_suitcaselab-default(089e9213-2b4c-4ba6-b0a0-3697da14c326)

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0
Deployment inbound-traffic: ScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-645cdc9f89 to 0
Pod inbound-traffic-ccf46495f-dqnv2: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-ccf46495f-dqnv2 to docker-desktop
Pod inbound-traffic-ccf46495f-dqnv2: Pulled - Container image "udp-test:v-b232e0fc4c" already present on machine
Pod inbound-traffic-ccf46495f-dqnv2: Created - Created container inbound-traffic
Pod inbound-traffic-ccf46495f-dqnv2: Started - Started container inbound-traffic
Pod inbound-traffic-ccf46495f-dqnv2: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-ccf46495f-dqnv2 ******
------ inbound-traffic ------bash: python: command not found


Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: >-
    CrashLoopBackOff - back-off 10s restarting failed container=inbound-traffic
    pod=inbound-traffic-ccf46495f-dqnv2_suitcaselab-default(089e9213-2b4c-4ba6-b0a0-3697da14c326)
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-645cdc9f89 to 0\e[39m\n\e[94mPod inbound-traffic-ccf46495f-dqnv2:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-ccf46495f-dqnv2 to docker-desktop\e[39m\n\e[94mPod inbound-traffic-ccf46495f-dqnv2:\e[39m \e[37mPulled - Container image \"udp-test:v-b232e0fc4c\" already present on machine\e[39m\n\e[94mPod inbound-traffic-ccf46495f-dqnv2:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-ccf46495f-dqnv2:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-ccf46495f-dqnv2:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-ccf46495f-dqnv2 ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39mbash: python: command not found\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '413664'
      generation: 6
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '6'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 599fa88b51ef30d6c10439070692092d9d28771bb670030360f6b1ada61a02a1
        garden.io/version: v-b232e0fc4c
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"599fa88b51ef30d6c10439070692092d9d28771bb670030360f6b1ada61a02a1","garden.io/version":"v-b232e0fc4c"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-b232e0fc4c"},{"name":"GARDEN_MODULE_VERSION","value":"v-b232e0fc4c"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-b232e0fc4c\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-b232e0fc4c","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 6
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:08:13.348Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:08:09.436Z
    completedAt: 2021-04-02T11:08:13.304Z
    batchId: 71b4f991-568e-4ab8-bb3c-5bc6ee351286
    version: v-b232e0fc4c


[2021-04-02T11:08:39.117Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0
Deployment inbound-traffic: ScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-ccf46495f to 0
Pod inbound-traffic-7f5f854bcc-9bjcp: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-7f5f854bcc-9bjcp to docker-desktop
Pod inbound-traffic-7f5f854bcc-9bjcp: Pulled - Container image "udp-test:v-aae74f2e5e" already present on machine
Pod inbound-traffic-7f5f854bcc-9bjcp: Created - Created container inbound-traffic
Pod inbound-traffic-7f5f854bcc-9bjcp: Started - Started container inbound-traffic
Pod inbound-traffic-7f5f854bcc-9bjcp: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-7f5f854bcc-9bjcp ******
------ inbound-traffic ------apt-get is a command line interface for retrieval of packages
and information about them from authenticated sources and
for installation, upgrade and removal of packages together
with their dependencies.

Most used commands:
  update - Retrieve new lists of packages
  upgrade - Perform an upgrade
  install - Install new packages (pkg is libc6 not libc6.deb)
  reinstall - Reinstall packages (pkg is libc6 not libc6.deb)
  remove - Remove packages
  purge - Remove packages and config files
  autoremove - Remove automatically all unused packages
  dist-upgrade - Distribution upgrade, see apt-get(8)
  dselect-upgrade - Follow dselect selections
  build-dep - Configure build-dependencies for source packages
  satisfy - Satisfy dependency strings
  clean - Erase downloaded archive files
  autoclean - Erase old downloaded archive files
  check - Verify that there are no broken dependencies
  source - Download source archives
  download - Download the binary package into the current directory
  changelog - Download and display the changelog for the given package

See apt-get(8) for more information about the available commands.
Configuration options and syntax is detailed in apt.conf(5).
Information about how to configure sources can be found in sources.list(5).
Package and version choices can be expressed via apt_preferences(5).
Security details are available in apt-secure(8).
                                        This APT has Super Cow Powers.


Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-ccf46495f to 0\e[39m\n\e[94mPod inbound-traffic-7f5f854bcc-9bjcp:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-7f5f854bcc-9bjcp to docker-desktop\e[39m\n\e[94mPod inbound-traffic-7f5f854bcc-9bjcp:\e[39m \e[37mPulled - Container image \"udp-test:v-aae74f2e5e\" already present on machine\e[39m\n\e[94mPod inbound-traffic-7f5f854bcc-9bjcp:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-7f5f854bcc-9bjcp:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-7f5f854bcc-9bjcp:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-7f5f854bcc-9bjcp ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39mapt-get is a command line interface for retrieval of packages\nand information about them from authenticated sources and\nfor installation, upgrade and removal of packages together\nwith their dependencies.\n\nMost used commands:\n  update - Retrieve new lists of packages\n  upgrade - Perform an upgrade\n  install - Install new packages (pkg is libc6 not libc6.deb)\n  reinstall - Reinstall packages (pkg is libc6 not libc6.deb)\n  remove - Remove packages\n  purge - Remove packages and config files\n  autoremove - Remove automatically all unused packages\n  dist-upgrade - Distribution upgrade, see apt-get(8)\n  dselect-upgrade - Follow dselect selections\n  build-dep - Configure build-dependencies for source packages\n  satisfy - Satisfy dependency strings\n  clean - Erase downloaded archive files\n  autoclean - Erase old downloaded archive files\n  check - Verify that there are no broken dependencies\n  source - Download source archives\n  download - Download the binary package into the current directory\n  changelog - Download and display the changelog for the given package\n\nSee apt-get(8) for more information about the available commands.\nConfiguration options and syntax is detailed in apt.conf(5).\nInformation about how to configure sources can be found in sources.list(5).\nPackage and version choices can be expressed via apt_preferences(5).\nSecurity details are available in apt-secure(8).\n                                        This APT has Super Cow Powers.\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '413763'
      generation: 7
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '7'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: cea4e4a183854f0e1eb7d22e6c3f6c28d42129574babcf8acc6a3f366ad69e62
        garden.io/version: v-aae74f2e5e
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"cea4e4a183854f0e1eb7d22e6c3f6c28d42129574babcf8acc6a3f366ad69e62","garden.io/version":"v-aae74f2e5e"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-aae74f2e5e"},{"name":"GARDEN_MODULE_VERSION","value":"v-aae74f2e5e"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-aae74f2e5e\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-aae74f2e5e","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 7
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:08:39.141Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:08:35.226Z
    completedAt: 2021-04-02T11:08:39.100Z
    batchId: 62777d03-d5d2-4444-8ea8-8b3eb0bbc2b5
    version: v-aae74f2e5e


[2021-04-02T11:09:51.480Z] Error deploying inbound-traffic: CrashLoopBackOff - back-off 10s restarting failed container=inbound-traffic pod=inbound-traffic-5b57767855-ndvs8_suitcaselab-default(599159d1-9dae-4b7c-bb52-26a95051ba86)

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0
Deployment inbound-traffic: ScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-7f5f854bcc to 0
Pod inbound-traffic-5b57767855-ndvs8: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-5b57767855-ndvs8 to docker-desktop
Pod inbound-traffic-5b57767855-ndvs8: Pulled - Container image "udp-test:v-b368554c57" already present on machine
Pod inbound-traffic-5b57767855-ndvs8: Created - Created container inbound-traffic
Pod inbound-traffic-5b57767855-ndvs8: Started - Started container inbound-traffic
Pod inbound-traffic-5b57767855-ndvs8: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-5b57767855-ndvs8 ******
------ inbound-traffic ------<no logs>

Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: >-
    CrashLoopBackOff - back-off 10s restarting failed container=inbound-traffic
    pod=inbound-traffic-5b57767855-ndvs8_suitcaselab-default(599159d1-9dae-4b7c-bb52-26a95051ba86)
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-7f5f854bcc to 0\e[39m\n\e[94mPod inbound-traffic-5b57767855-ndvs8:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-5b57767855-ndvs8 to docker-desktop\e[39m\n\e[94mPod inbound-traffic-5b57767855-ndvs8:\e[39m \e[37mPulled - Container image \"udp-test:v-b368554c57\" already present on machine\e[39m\n\e[94mPod inbound-traffic-5b57767855-ndvs8:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-5b57767855-ndvs8:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-5b57767855-ndvs8:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-5b57767855-ndvs8 ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39m<no logs>"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '413970'
      generation: 8
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '8'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 1d56b3d1174254fce5a9509f0e257e7a5e861d27a184d49a69c9d70dff418bc4
        garden.io/version: v-b368554c57
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"1d56b3d1174254fce5a9509f0e257e7a5e861d27a184d49a69c9d70dff418bc4","garden.io/version":"v-b368554c57"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-b368554c57"},{"name":"GARDEN_MODULE_VERSION","value":"v-b368554c57"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-b368554c57\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-b368554c57","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 8
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:09:51.505Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:09:47.623Z
    completedAt: 2021-04-02T11:09:51.468Z
    batchId: 56a1a33a-77f2-4a83-ae7c-b9cdde9ebfb7
    version: v-b368554c57


[2021-04-02T11:10:25.753Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0
Deployment inbound-traffic: ScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-5b57767855 to 0
Pod inbound-traffic-79c96bf58c-q8kn8: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-79c96bf58c-q8kn8 to docker-desktop
Pod inbound-traffic-79c96bf58c-q8kn8: Pulled - Container image "udp-test:v-a6b628e39d" already present on machine
Pod inbound-traffic-79c96bf58c-q8kn8: Created - Created container inbound-traffic
Pod inbound-traffic-79c96bf58c-q8kn8: Started - Started container inbound-traffic
Pod inbound-traffic-79c96bf58c-q8kn8: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-79c96bf58c-q8kn8 ******
------ inbound-traffic ------<no logs>

Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d484d7dd5 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-575db9885f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d484d7dd5 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-d48dcfd55 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-575db9885f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-6484d44f6f to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d48dcfd55 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-645cdc9f89 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-6484d44f6f to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - (combined from similar events): Scaled down replica set inbound-traffic-5b57767855 to 0\e[39m\n\e[94mPod inbound-traffic-79c96bf58c-q8kn8:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-79c96bf58c-q8kn8 to docker-desktop\e[39m\n\e[94mPod inbound-traffic-79c96bf58c-q8kn8:\e[39m \e[37mPulled - Container image \"udp-test:v-a6b628e39d\" already present on machine\e[39m\n\e[94mPod inbound-traffic-79c96bf58c-q8kn8:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-79c96bf58c-q8kn8:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-79c96bf58c-q8kn8:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-79c96bf58c-q8kn8 ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39m<no logs>"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '414086'
      generation: 9
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '9'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 74b94b5a8f9b415a78c3d062bc8739c8333048404a3d539a1bf94f5a41ca0aaf
        garden.io/version: v-a6b628e39d
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"74b94b5a8f9b415a78c3d062bc8739c8333048404a3d539a1bf94f5a41ca0aaf","garden.io/version":"v-a6b628e39d"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-a6b628e39d"},{"name":"GARDEN_MODULE_VERSION","value":"v-a6b628e39d"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-a6b628e39d\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-a6b628e39d","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 9
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T11:10:25.776Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T11:10:21.916Z
    completedAt: 2021-04-02T11:10:25.745Z
    batchId: a2b18bd0-3d77-4a4d-9e52-5592ff392b54
    version: v-a6b628e39d


[2021-04-02T18:40:14.681Z] Error deploying inbound-traffic: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-7ff7b8d49 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-d7bbc66c9 to 0
Deployment inbound-traffic: ScalingReplicaSet - Scaled up replica set inbound-traffic-684c478f89 to 1
Deployment inbound-traffic: ScalingReplicaSet - Scaled down replica set inbound-traffic-7ff7b8d49 to 0
Pod inbound-traffic-684c478f89-ms6qd: Scheduled - Successfully assigned suitcaselab-default/inbound-traffic-684c478f89-ms6qd to docker-desktop
Pod inbound-traffic-684c478f89-ms6qd: Pulled - Container image "udp-test:v-925dc3764b" already present on machine
Pod inbound-traffic-684c478f89-ms6qd: Created - Created container inbound-traffic
Pod inbound-traffic-684c478f89-ms6qd: Started - Started container inbound-traffic
Pod inbound-traffic-684c478f89-ms6qd: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic

****** inbound-traffic-684c478f89-ms6qd ******
------ inbound-traffic ------  File "roaddetect.py", line 515
    print("header info:", header.playerCarIndex, header.)
                                                        ^
SyntaxError: invalid syntax


Error Details:

serviceName: inbound-traffic
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-7ff7b8d49 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-d7bbc66c9 to 0\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled up replica set inbound-traffic-684c478f89 to 1\e[39m\n\e[94mDeployment inbound-traffic:\e[39m \e[37mScalingReplicaSet - Scaled down replica set inbound-traffic-7ff7b8d49 to 0\e[39m\n\e[94mPod inbound-traffic-684c478f89-ms6qd:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/inbound-traffic-684c478f89-ms6qd to docker-desktop\e[39m\n\e[94mPod inbound-traffic-684c478f89-ms6qd:\e[39m \e[37mPulled - Container image \"udp-test:v-925dc3764b\" already present on machine\e[39m\n\e[94mPod inbound-traffic-684c478f89-ms6qd:\e[39m \e[37mCreated - Created container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-684c478f89-ms6qd:\e[39m \e[37mStarted - Started container inbound-traffic\e[39m\n\e[94mPod inbound-traffic-684c478f89-ms6qd:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=docker-desktop logs deployment/inbound-traffic\e[39m\n\e[94m\e[39m\n\e[94m****** inbound-traffic-684c478f89-ms6qd ******\e[39m\n\e[94m\e[39m\e[90m------ inbound-traffic ------\e[39m  File \"roaddetect.py\", line 515\n    print(\"header info:\", header.playerCarIndex, header.)\n                                                        ^\nSyntaxError: invalid syntax\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: inbound-traffic
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/inbound-traffic
      uid: 2c53e763-a05b-437c-9553-467b1d5f55b8
      resourceVersion: '441592'
      generation: 14
      creationTimestamp: '2021-04-02T11:02:59Z'
      labels:
        module: udp-test
        service: inbound-traffic
      annotations:
        deployment.kubernetes.io/revision: '14'
        garden.io/configured.replicas: '1'
        garden.io/generated: 'true'
        garden.io/manifest-hash: 4c459fa3e9e7cff21b174642eef496d974e4fcffa940f7763dac3d420b7b8a1a
        garden.io/version: v-925dc3764b
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/configured.replicas":"1","garden.io/generated":"true","garden.io/manifest-hash":"4c459fa3e9e7cff21b174642eef496d974e4fcffa940f7763dac3d420b7b8a1a","garden.io/version":"v-925dc3764b"},"labels":{"module":"udp-test","service":"inbound-traffic"},"name":"inbound-traffic","namespace":"suitcaselab-default"},"spec":{"replicas":1,"revisionHistoryLimit":3,"selector":{"matchLabels":{"service":"inbound-traffic"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"module":"udp-test","service":"inbound-traffic"}},"spec":{"containers":[{"env":[{"name":"GARDEN_VERSION","value":"v-925dc3764b"},{"name":"GARDEN_MODULE_VERSION","value":"v-925dc3764b"},{"name":"GARDEN_DEPENDENCIES","value":"[{\"moduleName\":\"udp-test\",\"name\":\"udp-test\",\"type\":\"build\",\"version\":\"v-925dc3764b\"}]"},{"name":"POD_HOST_IP","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_SERVICE_ACCOUNT","valueFrom":{"fieldRef":{"fieldPath":"spec.serviceAccountName"}}},{"name":"POD_UID","valueFrom":{"fieldRef":{"fieldPath":"metadata.uid"}}}],"image":"udp-test:v-925dc3764b","imagePullPolicy":"IfNotPresent","name":"inbound-traffic","ports":[{"containerPort":20777,"name":"udp","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"10m","memory":"90Mi"}},"securityContext":{"allowPrivilegeEscalation":false}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
      managedFields: []
    spec:
      replicas: 1
      selector:
        matchLabels:
          service: inbound-traffic
      template:
        metadata:
          creationTimestamp: null
          labels:
            module: udp-test
            service: inbound-traffic
        spec:
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 5
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      revisionHistoryLimit: 3
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 14
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-04-02T18:40:14.704Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.inbound-traffic:
    type: deploy
    description: deploying service 'inbound-traffic' (from module 'udp-test')
    key: deploy.inbound-traffic
    name: inbound-traffic
    startedAt: 2021-04-02T18:40:10.779Z
    completedAt: 2021-04-02T18:40:14.673Z
    batchId: 6e54603f-c85f-44fe-8197-f79ea1aa0408
    version: v-925dc3764b


[2021-04-14T02:41:09.961Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
all: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T02:41:59.940Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T02:41:04.685Z
    completedAt: 2021-04-14T02:41:09.956Z
    batchId: 0a6705b0-4571-4484-9566-71a0f571aa4a
    version: v-cd25dd0295


[2021-04-14T16:44:58.192Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

Error from server (Invalid): error when creating "STDIN": Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error from server (Invalid): error when creating "STDIN": Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (Invalid): error when creating "STDIN": Service
  "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001:
  provided port is already allocated

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
all: >-
  Error from server (Invalid): error when creating "STDIN": Service
  "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001:
  provided port is already allocated

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T16:44:59.905Z] Unable to run docker command: Command "/Users/mattgunter/.garden/tools/docker/fa0336ac33b46eb5/docker/docker push gcr.io/th-structure-flow-demo/udp-test:v-fd19b4b76a" failed with code 1:

unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Here's the full output:

The push refers to repository [gcr.io/th-structure-flow-demo/udp-test]
81f1d23b9a4a: Preparing
72672abdbbc2: Preparing
e709c172d615: Preparing
62943701142e: Preparing
346be19f13b0: Preparing
935f303ebf75: Preparing
0e64bafdc7ee: Preparing
935f303ebf75: Waiting
0e64bafdc7ee: Waiting
unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Error Details:


args: []
cwd: >-
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/udp-test


[2021-04-14T16:47:37.922Z] Unable to run docker command: Command "/Users/mattgunter/.garden/tools/docker/fa0336ac33b46eb5/docker/docker push gcr.io/th-structure-flow-demo/udp-test:v-fd19b4b76a" failed with code 1:

unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Here's the full output:

The push refers to repository [gcr.io/th-structure-flow-demo/udp-test]
81f1d23b9a4a: Preparing
72672abdbbc2: Preparing
e709c172d615: Preparing
62943701142e: Preparing
346be19f13b0: Preparing
935f303ebf75: Preparing
0e64bafdc7ee: Preparing
935f303ebf75: Waiting
0e64bafdc7ee: Waiting
unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Error Details:


args: []
cwd: >-
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/udp-test


[2021-04-14T16:47:39.881Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
all: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T16:47:39.891Z] 2 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T16:47:36.274Z
    completedAt: 2021-04-14T16:47:39.873Z
    batchId: 1f74dc8a-c9bf-4828-8654-841b7c0c60aa
    version: v-cd25dd0295
  build.udp-test:
    type: build
    description: building udp-test
    key: build.udp-test
    name: udp-test
    startedAt: 2021-04-14T16:47:34.931Z
    completedAt: 2021-04-14T16:47:37.917Z
    batchId: 940b80ab-702a-4f49-b505-ab29c5632893
    version: v-fd19b4b76a


[2021-04-14T16:52:55.510Z] Unable to run docker command: Command "/Users/mattgunter/.garden/tools/docker/fa0336ac33b46eb5/docker/docker push gcr.io/th-structure-flow-demo/udp-test:v-fd19b4b76a" failed with code 1:

unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Here's the full output:

The push refers to repository [gcr.io/th-structure-flow-demo/udp-test]
81f1d23b9a4a: Preparing
72672abdbbc2: Preparing
e709c172d615: Preparing
62943701142e: Preparing
346be19f13b0: Preparing
935f303ebf75: Preparing
0e64bafdc7ee: Preparing
935f303ebf75: Waiting
0e64bafdc7ee: Waiting
unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Error Details:


args: []
cwd: >-
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/udp-test


[2021-04-14T16:52:58.150Z] 1 deploy task(s) failed!

Error Details:

results:
  build.udp-test:
    type: build
    description: building udp-test
    key: build.udp-test
    name: udp-test
    startedAt: 2021-04-14T16:52:51.983Z
    completedAt: 2021-04-14T16:52:55.505Z
    batchId: e2e4dac1-71a9-4391-923b-907efab16fa9
    version: v-fd19b4b76a


[2021-04-14T17:06:47.798Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
all: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T17:13:46.481Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is immutable after creation except resources.requests for bound claims

The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is immutable after creation except resources.requests for bound claims

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is
  immutable after creation except resources.requests for bound claims
all: >-
  The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is
  immutable after creation except resources.requests for bound claims
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T17:13:46.886Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=recorder --output=json -f -" failed with code 1:

The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is immutable after creation except resources.requests for bound claims

The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is immutable after creation except resources.requests for bound claims

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is
  immutable after creation except resources.requests for bound claims
all: >-
  The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is
  immutable after creation except resources.requests for bound claims
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T17:13:46.896Z] 2 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T17:13:45.884Z
    completedAt: 2021-04-14T17:13:46.475Z
    batchId: 1195f10a-0607-414e-b5c3-e32d0df0a173
    version: v-2dd5493fd9
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-14T17:13:46.045Z
    completedAt: 2021-04-14T17:13:46.881Z
    batchId: 69f1b6f7-4765-4dd4-a88e-ec62acfd6c2e
    version: v-7106495b56


[2021-04-14T17:15:35.352Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
all: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T17:15:41.468Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T17:15:34.350Z
    completedAt: 2021-04-14T17:15:35.346Z
    batchId: df707de7-4550-4b1e-8e6c-fcc1d0ea4c6d
    version: v-2dd5493fd9


[2021-04-14T17:24:30.727Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=recorder --output=json -f -" failed with code 1:

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
all: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T17:24:36.781Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-14T17:24:29.722Z
    completedAt: 2021-04-14T17:24:30.721Z
    batchId: 04cae7d1-8a00-4f64-aff8-d2699128c7f7
    version: v-8f64819d44


[2021-04-14T17:52:59.514Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=recorder --output=json -f -" failed with code 1:

The PersistentVolume "data-pv-volume" is invalid: spec.accessModes: Unsupported value: "ReadWrite": supported values: "ReadOnlyMany", "ReadWriteMany", "ReadWriteOnce"

The PersistentVolume "data-pv-volume" is invalid: spec.accessModes: Unsupported value: "ReadWrite": supported values: "ReadOnlyMany", "ReadWriteMany", "ReadWriteOnce"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The PersistentVolume "data-pv-volume" is invalid: spec.accessModes:
  Unsupported value: "ReadWrite": supported values: "ReadOnlyMany",
  "ReadWriteMany", "ReadWriteOnce"
all: >-
  The PersistentVolume "data-pv-volume" is invalid: spec.accessModes:
  Unsupported value: "ReadWrite": supported values: "ReadOnlyMany",
  "ReadWriteMany", "ReadWriteOnce"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T17:52:59.529Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

Error from server (Invalid): error when creating "STDIN": PersistentVolume "data-pv-volume" is invalid: spec.accessModes: Unsupported value: "ReadWrite": supported values: "ReadOnlyMany", "ReadWriteMany", "ReadWriteOnce"
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error from server (Invalid): error when creating "STDIN": PersistentVolume "data-pv-volume" is invalid: spec.accessModes: Unsupported value: "ReadWrite": supported values: "ReadOnlyMany", "ReadWriteMany", "ReadWriteOnce"
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (Invalid): error when creating "STDIN": PersistentVolume
  "data-pv-volume" is invalid: spec.accessModes: Unsupported value: "ReadWrite":
  supported values: "ReadOnlyMany", "ReadWriteMany", "ReadWriteOnce"

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
all: >-
  Error from server (Invalid): error when creating "STDIN": PersistentVolume
  "data-pv-volume" is invalid: spec.accessModes: Unsupported value: "ReadWrite":
  supported values: "ReadOnlyMany", "ReadWriteMany", "ReadWriteOnce"

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T17:52:59.538Z] 2 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T17:52:52.222Z
    completedAt: 2021-04-14T17:52:59.523Z
    batchId: c1437f77-2ccc-4fdd-b705-1f19dc644615
    version: v-9421bc46ca
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-14T17:52:52.231Z
    completedAt: 2021-04-14T17:52:59.509Z
    batchId: 4a6824ce-170a-4045-a84b-02032fa17468
    version: v-2b966ba8c8


[2021-04-14T18:12:06.966Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=recorder --output=json -f -" failed with code 1:

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error from server (AlreadyExists): error when creating "STDIN": persistentvolumes "data-pv-volume" already exists
Error from server (AlreadyExists): error when creating "STDIN": persistentvolumeclaims "data-pv-claim" already exists

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=recorder --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
all: >-
  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumes "data-pv-volume" already exists

  Error from server (AlreadyExists): error when creating "STDIN":
  persistentvolumeclaims "data-pv-claim" already exists
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T18:12:10.435Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-14T18:12:05.107Z
    completedAt: 2021-04-14T18:12:06.960Z
    batchId: 466b6232-4945-4edf-8e28-0d550fdf0bbc
    version: v-ad78ad9b94


[2021-04-14T19:27:28.102Z] contexts[0].context.cluster is missing

[2021-04-14T19:27:28.108Z] Failed resolving one or more providers:
- local-kubernetes

Error Details:

rawConfigs: []
taskResults:
  resolve-provider.exec:
    type: resolve-provider
    key: resolve-provider.exec
    name: exec
    description: resolving provider exec
    completedAt: 2021-04-14T19:27:27.349Z
    batchId: 87543c01-0661-4438-acaf-f54220429be0
    output:
      name: exec
      dependencies: {}
      moduleConfigs: []
      config:
        name: exec
        dependencies: []
        path: /Users/mattgunter/Projects/Stream-test-demo/garden-projects
      status:
        ready: true
        outputs: {}
      dashboardPages: []
    dependencyResults: {}
    version: 0.12.19
    startedAt: 2021-04-14T19:27:27.327Z
  resolve-provider.container:
    type: resolve-provider
    key: resolve-provider.container
    name: container
    description: resolving provider container
    completedAt: 2021-04-14T19:27:27.345Z
    batchId: 1c4f2d2f-1ea6-44c8-8b81-710f7d5e97a0
    output:
      name: container
      dependencies: {}
      moduleConfigs: []
      config:
        name: container
        dependencies: []
        path: /Users/mattgunter/Projects/Stream-test-demo/garden-projects
      status:
        ready: true
        outputs: {}
      dashboardPages: []
    dependencyResults: {}
    version: 0.12.19
    startedAt: 2021-04-14T19:27:27.328Z
  resolve-provider.local-kubernetes:
    type: resolve-provider
    description: resolving provider local-kubernetes
    key: resolve-provider.local-kubernetes
    name: local-kubernetes
    startedAt: 2021-04-14T19:27:27.346Z
    completedAt: 2021-04-14T19:27:28.058Z
    batchId: 1c4f2d2f-1ea6-44c8-8b81-710f7d5e97a0
    version: 0.12.19
  resolve-provider.templated:
    type: resolve-provider
    key: resolve-provider.templated
    name: templated
    description: resolving provider templated
    completedAt: 2021-04-14T19:27:27.348Z
    batchId: 6e911875-413f-4bfb-a173-f1c9a1294b3d
    output:
      name: templated
      dependencies: {}
      moduleConfigs: []
      config:
        name: templated
        path: /Users/mattgunter/Projects/Stream-test-demo/garden-projects
      status:
        ready: true
        outputs: {}
      dashboardPages: []
    dependencyResults: {}
    version: 0.12.19
    startedAt: 2021-04-14T19:27:27.329Z
messages: []


[2021-04-14T19:31:26.299Z] contexts[0].context.cluster is missing

[2021-04-14T19:31:26.304Z] Failed resolving one or more providers:
- local-kubernetes

Error Details:

rawConfigs: []
taskResults:
  resolve-provider.exec:
    type: resolve-provider
    key: resolve-provider.exec
    name: exec
    description: resolving provider exec
    completedAt: 2021-04-14T19:31:25.611Z
    batchId: e52a6028-1aec-4c8b-9447-f71bd9af3b80
    output:
      name: exec
      dependencies: {}
      moduleConfigs: []
      config:
        name: exec
        dependencies: []
        path: /Users/mattgunter/Projects/Stream-test-demo/garden-projects
      status:
        ready: true
        outputs: {}
        cached: true
      dashboardPages: []
    dependencyResults: {}
    version: 0.12.19
    startedAt: 2021-04-14T19:31:25.593Z
  resolve-provider.container:
    type: resolve-provider
    key: resolve-provider.container
    name: container
    description: resolving provider container
    completedAt: 2021-04-14T19:31:25.607Z
    batchId: ed8b6fff-0e85-43df-a916-aa181c1414ae
    output:
      name: container
      dependencies: {}
      moduleConfigs: []
      config:
        name: container
        dependencies: []
        path: /Users/mattgunter/Projects/Stream-test-demo/garden-projects
      status:
        ready: true
        outputs: {}
        cached: true
      dashboardPages: []
    dependencyResults: {}
    version: 0.12.19
    startedAt: 2021-04-14T19:31:25.594Z
  resolve-provider.local-kubernetes:
    type: resolve-provider
    description: resolving provider local-kubernetes
    key: resolve-provider.local-kubernetes
    name: local-kubernetes
    startedAt: 2021-04-14T19:31:25.608Z
    completedAt: 2021-04-14T19:31:26.266Z
    batchId: ed8b6fff-0e85-43df-a916-aa181c1414ae
    version: 0.12.19
  resolve-provider.templated:
    type: resolve-provider
    key: resolve-provider.templated
    name: templated
    description: resolving provider templated
    completedAt: 2021-04-14T19:31:25.612Z
    batchId: 0127b9da-0ff8-421a-8c0e-1a1293ef547c
    output:
      name: templated
      dependencies: {}
      moduleConfigs: []
      config:
        name: templated
        path: /Users/mattgunter/Projects/Stream-test-demo/garden-projects
      status:
        ready: true
        outputs: {}
        cached: true
      dashboardPages: []
    dependencyResults: {}
    version: 0.12.19
    startedAt: 2021-04-14T19:31:25.595Z
messages: []


[2021-04-14T19:34:00.062Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874\",\"pv.beta.kubernetes.io/gid\":\"1234\",\"service\":\"player\"},\"labels\":{\"service\":\"player\",\"type\":\"local\"},\"name\":\"data-pv-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"capacity\":{\"storage\":\"10Gi\"},\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}
"}},"spec":{"hostPath":{"path":"/mnt/disks/data"},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumes", GroupVersionKind: "/v1, Kind=PersistentVolume"
Name: "data-pv-volume", Namespace: ""
for: "STDIN": PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation
core.PersistentVolumeSource{
	GCEPersistentDisk:    nil,
	AWSElasticBlockStore: nil,
	HostPath: &core.HostPathVolumeSource{
- 		Path: "/mnt/disks/data",
+ 		Path: "/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data",
		Type: &"",
	},
	Glusterfs: nil,
	NFS:       nil,
	... // 17 identical fields
}

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541\",\"service\":\"player\"},\"labels\":{\"service\":\"player\"},\"name\":\"data-pv-claim\",\"namespace\":\"suitcaselab-default\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}
"}},"spec":{"resources":{"requests":{"storage":"5Gi"}},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumeclaims", GroupVersionKind: "/v1, Kind=PersistentVolumeClaim"
Name: "data-pv-claim", Namespace: "suitcaselab-default"
for: "STDIN": PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims
core.PersistentVolumeClaimSpec{
	... // 2 identical fields
	Resources:        core.ResourceRequirements{Requests: core.ResourceList{s"storage": {i: resource.int64Amount{value: 3221225472}, s: "3Gi", Format: "BinarySI"}}},
	VolumeName:       "data-pv-volume",
- 	StorageClassName: &"standard",
+ 	StorageClassName: &"manual",
	VolumeMode:       &"Filesystem",
	DataSource:       nil,
}

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874\",\"pv.beta.kubernetes.io/gid\":\"1234\",\"service\":\"player\"},\"labels\":{\"service\":\"player\",\"type\":\"local\"},\"name\":\"data-pv-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"capacity\":{\"storage\":\"10Gi\"},\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}\n"}},"spec":{"hostPath":{"path":"/mnt/disks/data"},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumes", GroupVersionKind: "/v1, Kind=PersistentVolume"
Name: "data-pv-volume", Namespace: ""
for: "STDIN": PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation
  core.PersistentVolumeSource{
  	GCEPersistentDisk:    nil,
  	AWSElasticBlockStore: nil,
  	HostPath: &core.HostPathVolumeSource{
- 		Path: "/mnt/disks/data",
+ 		Path: "/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data",
  		Type: &"",
  	},
  	Glusterfs: nil,
  	NFS:       nil,
  	... // 17 identical fields
  }

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541\",\"service\":\"player\"},\"labels\":{\"service\":\"player\"},\"name\":\"data-pv-claim\",\"namespace\":\"suitcaselab-default\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}\n"}},"spec":{"resources":{"requests":{"storage":"5Gi"}},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumeclaims", GroupVersionKind: "/v1, Kind=PersistentVolumeClaim"
Name: "data-pv-claim", Namespace: "suitcaselab-default"
for: "STDIN": PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims
  core.PersistentVolumeClaimSpec{
  	... // 2 identical fields
  	Resources:        core.ResourceRequirements{Requests: core.ResourceList{s"storage": {i: resource.int64Amount{value: 3221225472}, s: "3Gi", Format: "BinarySI"}}},
  	VolumeName:       "data-pv-volume",
- 	StorageClassName: &"standard",
+ 	StorageClassName: &"manual",
  	VolumeMode:       &"Filesystem",
  	DataSource:       nil,
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "Error from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolume\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874\\\",\\\"pv.beta.kubernetes.io/gid\\\":\\\"1234\\\",\\\"service\\\":\\\"player\\\"},\\\"labels\\\":{\\\"service\\\":\\\"player\\\",\\\"type\\\":\\\"local\\\"},\\\"name\\\":\\\"data-pv-volume\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"capacity\\\":{\\\"storage\\\":\\\"10Gi\\\"},\\\"hostPath\\\":{\\\"path\\\":\\\"/mnt/disks/data\\\"},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\"}},\"spec\":{\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumes\", GroupVersionKind: \"/v1, Kind=PersistentVolume\"\nName: \"data-pv-volume\", Namespace: \"\"\nfor: \"STDIN\": PersistentVolume \"data-pv-volume\" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation\n  core.PersistentVolumeSource{\n  \tGCEPersistentDisk:    nil,\n  \tAWSElasticBlockStore: nil,\n  \tHostPath: &core.HostPathVolumeSource{\n- \t\tPath: \"/mnt/disks/data\",\n+ \t\tPath: \"/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data\",\n  \t\tType: &\"\",\n  \t},\n  \tGlusterfs: nil,\n  \tNFS:       nil,\n  \t... // 17 identical fields\n  }\n\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolumeClaim\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541\\\",\\\"service\\\":\\\"player\\\"},\\\"labels\\\":{\\\"service\\\":\\\"player\\\"},\\\"name\\\":\\\"data-pv-claim\\\",\\\"namespace\\\":\\\"suitcaselab-default\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"storage\\\":\\\"5Gi\\\"}},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\"}},\"spec\":{\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumeclaims\", GroupVersionKind: \"/v1, Kind=PersistentVolumeClaim\"\nName: \"data-pv-claim\", Namespace: \"suitcaselab-default\"\nfor: \"STDIN\": PersistentVolumeClaim \"data-pv-claim\" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims\n  core.PersistentVolumeClaimSpec{\n  \t... // 2 identical fields\n  \tResources:        core.ResourceRequirements{Requests: core.ResourceList{s\"storage\": {i: resource.int64Amount{value: 3221225472}, s: \"3Gi\", Format: \"BinarySI\"}}},\n  \tVolumeName:       \"data-pv-volume\",\n- \tStorageClassName: &\"standard\",\n+ \tStorageClassName: &\"manual\",\n  \tVolumeMode:       &\"Filesystem\",\n  \tDataSource:       nil,\n  }\n"
all: "Error from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolume\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"b9a9d16162ac2d3e2b1721fbc4c5ea6b61a636d90a5c51e980a494f24b117874\\\",\\\"pv.beta.kubernetes.io/gid\\\":\\\"1234\\\",\\\"service\\\":\\\"player\\\"},\\\"labels\\\":{\\\"service\\\":\\\"player\\\",\\\"type\\\":\\\"local\\\"},\\\"name\\\":\\\"data-pv-volume\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"capacity\\\":{\\\"storage\\\":\\\"10Gi\\\"},\\\"hostPath\\\":{\\\"path\\\":\\\"/mnt/disks/data\\\"},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\"}},\"spec\":{\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumes\", GroupVersionKind: \"/v1, Kind=PersistentVolume\"\nName: \"data-pv-volume\", Namespace: \"\"\nfor: \"STDIN\": PersistentVolume \"data-pv-volume\" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation\n  core.PersistentVolumeSource{\n  \tGCEPersistentDisk:    nil,\n  \tAWSElasticBlockStore: nil,\n  \tHostPath: &core.HostPathVolumeSource{\n- \t\tPath: \"/mnt/disks/data\",\n+ \t\tPath: \"/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data\",\n  \t\tType: &\"\",\n  \t},\n  \tGlusterfs: nil,\n  \tNFS:       nil,\n  \t... // 17 identical fields\n  }\n\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolumeClaim\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"eeacd04015a3bdd88f884eb6e6acc585e8a78dceb86f5906463b1a1a6de7e541\\\",\\\"service\\\":\\\"player\\\"},\\\"labels\\\":{\\\"service\\\":\\\"player\\\"},\\\"name\\\":\\\"data-pv-claim\\\",\\\"namespace\\\":\\\"suitcaselab-default\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"storage\\\":\\\"5Gi\\\"}},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\"}},\"spec\":{\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumeclaims\", GroupVersionKind: \"/v1, Kind=PersistentVolumeClaim\"\nName: \"data-pv-claim\", Namespace: \"suitcaselab-default\"\nfor: \"STDIN\": PersistentVolumeClaim \"data-pv-claim\" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims\n  core.PersistentVolumeClaimSpec{\n  \t... // 2 identical fields\n  \tResources:        core.ResourceRequirements{Requests: core.ResourceList{s\"storage\": {i: resource.int64Amount{value: 3221225472}, s: \"3Gi\", Format: \"BinarySI\"}}},\n  \tVolumeName:       \"data-pv-volume\",\n- \tStorageClassName: &\"standard\",\n+ \tStorageClassName: &\"manual\",\n  \tVolumeMode:       &\"Filesystem\",\n  \tDataSource:       nil,\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T19:34:00.119Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=recorder --output=json -f -" failed with code 1:

  Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae\",\"service\":\"recorder\"},\"labels\":{\"app\":\"f1-recorder\",\"service\":\"recorder\"},\"name\":\"recorder-pod\",\"namespace\":\"suitcaselab-default\"},\"spec\":{\"containers\":[{\"image\":\"mgunter/python-f1-record\",\"name\":\"f1-recorder\",\"ports\":[{\"containerPort\":20777,\"name\":\"recorder\"}],\"volumeMounts\":[{\"mountPath\":\"/sensable\",\"name\":\"data-pv-storage\"}]}],\"volumes\":[{\"name\":\"data-pv-storage\",\"persistentVolumeClaim\":{\"claimName\":\"data-pv-claim\"}}]}}
"}},"spec":{"$setElementOrder/containers":[{"name":"f1-recorder"}],"$setElementOrder/volumes":[{"name":"data-pv-storage"}],"containers":[{"$setElementOrder/ports":[{"containerPort":20777}],"$setElementOrder/volumeMounts":[{"mountPath":"/sensable"}],"name":"f1-recorder","ports":[{"containerPort":20777,"name":"recorder"}]}]}}
to:
Resource: "/v1, Resource=pods", GroupVersionKind: "/v1, Kind=Pod"
Name: "recorder-pod", Namespace: "suitcaselab-default"
for: "STDIN": Pod "recorder-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-6sljp", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-6sljp", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			... // 3 identical fields
			Args:       nil,
			WorkingDir: "",
			Ports: []core.ContainerPort{
				{
- 					Name:          "recorder",
+ 					Name:          "http-server",
					HostPort:      0,
					ContainerPort: 20777,
					... // 2 identical fields
				},
			},
			EnvFrom: nil,
			Env:     nil,
			... // 14 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052\",\"pv.beta.kubernetes.io/gid\":\"1234\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\",\"type\":\"local\"},\"name\":\"data-pv-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"capacity\":{\"storage\":\"10Gi\"},\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}
","service":"recorder"},"labels":{"service":"recorder"}},"spec":{"hostPath":{"path":"/mnt/disks/data"},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumes", GroupVersionKind: "/v1, Kind=PersistentVolume"
Name: "data-pv-volume", Namespace: ""
for: "STDIN": PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation
core.PersistentVolumeSource{
	GCEPersistentDisk:    nil,
	AWSElasticBlockStore: nil,
	HostPath: &core.HostPathVolumeSource{
- 		Path: "/mnt/disks/data",
+ 		Path: "/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data",
		Type: &"",
	},
	Glusterfs: nil,
	NFS:       nil,
	... // 17 identical fields
}

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\"},\"name\":\"data-pv-claim\",\"namespace\":\"suitcaselab-default\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}
","service":"recorder"},"labels":{"service":"recorder"}},"spec":{"resources":{"requests":{"storage":"5Gi"}},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumeclaims", GroupVersionKind: "/v1, Kind=PersistentVolumeClaim"
Name: "data-pv-claim", Namespace: "suitcaselab-default"
for: "STDIN": PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims
core.PersistentVolumeClaimSpec{
	... // 2 identical fields
	Resources:        core.ResourceRequirements{Requests: core.ResourceList{s"storage": {i: resource.int64Amount{value: 3221225472}, s: "3Gi", Format: "BinarySI"}}},
	VolumeName:       "data-pv-volume",
- 	StorageClassName: &"standard",
+ 	StorageClassName: &"manual",
	VolumeMode:       &"Filesystem",
	DataSource:       nil,
}

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae\",\"service\":\"recorder\"},\"labels\":{\"app\":\"f1-recorder\",\"service\":\"recorder\"},\"name\":\"recorder-pod\",\"namespace\":\"suitcaselab-default\"},\"spec\":{\"containers\":[{\"image\":\"mgunter/python-f1-record\",\"name\":\"f1-recorder\",\"ports\":[{\"containerPort\":20777,\"name\":\"recorder\"}],\"volumeMounts\":[{\"mountPath\":\"/sensable\",\"name\":\"data-pv-storage\"}]}],\"volumes\":[{\"name\":\"data-pv-storage\",\"persistentVolumeClaim\":{\"claimName\":\"data-pv-claim\"}}]}}\n"}},"spec":{"$setElementOrder/containers":[{"name":"f1-recorder"}],"$setElementOrder/volumes":[{"name":"data-pv-storage"}],"containers":[{"$setElementOrder/ports":[{"containerPort":20777}],"$setElementOrder/volumeMounts":[{"mountPath":"/sensable"}],"name":"f1-recorder","ports":[{"containerPort":20777,"name":"recorder"}]}]}}
to:
Resource: "/v1, Resource=pods", GroupVersionKind: "/v1, Kind=Pod"
Name: "recorder-pod", Namespace: "suitcaselab-default"
for: "STDIN": Pod "recorder-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-6sljp", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-6sljp", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 3 identical fields
  			Args:       nil,
  			WorkingDir: "",
  			Ports: []core.ContainerPort{
  				{
- 					Name:          "recorder",
+ 					Name:          "http-server",
  					HostPort:      0,
  					ContainerPort: 20777,
  					... // 2 identical fields
  				},
  			},
  			EnvFrom: nil,
  			Env:     nil,
  			... // 14 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052\",\"pv.beta.kubernetes.io/gid\":\"1234\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\",\"type\":\"local\"},\"name\":\"data-pv-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"capacity\":{\"storage\":\"10Gi\"},\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}\n","service":"recorder"},"labels":{"service":"recorder"}},"spec":{"hostPath":{"path":"/mnt/disks/data"},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumes", GroupVersionKind: "/v1, Kind=PersistentVolume"
Name: "data-pv-volume", Namespace: ""
for: "STDIN": PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation
  core.PersistentVolumeSource{
  	GCEPersistentDisk:    nil,
  	AWSElasticBlockStore: nil,
  	HostPath: &core.HostPathVolumeSource{
- 		Path: "/mnt/disks/data",
+ 		Path: "/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data",
  		Type: &"",
  	},
  	Glusterfs: nil,
  	NFS:       nil,
  	... // 17 identical fields
  }

Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"garden.io/manifest-hash":"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\"},\"name\":\"data-pv-claim\",\"namespace\":\"suitcaselab-default\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}\n","service":"recorder"},"labels":{"service":"recorder"}},"spec":{"resources":{"requests":{"storage":"5Gi"}},"storageClassName":"standard"}}
to:
Resource: "/v1, Resource=persistentvolumeclaims", GroupVersionKind: "/v1, Kind=PersistentVolumeClaim"
Name: "data-pv-claim", Namespace: "suitcaselab-default"
for: "STDIN": PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims
  core.PersistentVolumeClaimSpec{
  	... // 2 identical fields
  	Resources:        core.ResourceRequirements{Requests: core.ResourceList{s"storage": {i: resource.int64Amount{value: 3221225472}, s: "3Gi", Format: "BinarySI"}}},
  	VolumeName:       "data-pv-volume",
- 	StorageClassName: &"standard",
+ 	StorageClassName: &"manual",
  	VolumeMode:       &"Filesystem",
  	DataSource:       nil,
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
exitCode: 1
stdout: ''
stderr: "Error from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Pod\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae\\\",\\\"service\\\":\\\"recorder\\\"},\\\"labels\\\":{\\\"app\\\":\\\"f1-recorder\\\",\\\"service\\\":\\\"recorder\\\"},\\\"name\\\":\\\"recorder-pod\\\",\\\"namespace\\\":\\\"suitcaselab-default\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"image\\\":\\\"mgunter/python-f1-record\\\",\\\"name\\\":\\\"f1-recorder\\\",\\\"ports\\\":[{\\\"containerPort\\\":20777,\\\"name\\\":\\\"recorder\\\"}],\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/sensable\\\",\\\"name\\\":\\\"data-pv-storage\\\"}]}],\\\"volumes\\\":[{\\\"name\\\":\\\"data-pv-storage\\\",\\\"persistentVolumeClaim\\\":{\\\"claimName\\\":\\\"data-pv-claim\\\"}}]}}\\n\"}},\"spec\":{\"$setElementOrder/containers\":[{\"name\":\"f1-recorder\"}],\"$setElementOrder/volumes\":[{\"name\":\"data-pv-storage\"}],\"containers\":[{\"$setElementOrder/ports\":[{\"containerPort\":20777}],\"$setElementOrder/volumeMounts\":[{\"mountPath\":\"/sensable\"}],\"name\":\"f1-recorder\",\"ports\":[{\"containerPort\":20777,\"name\":\"recorder\"}]}]}}\nto:\nResource: \"/v1, Resource=pods\", GroupVersionKind: \"/v1, Kind=Pod\"\nName: \"recorder-pod\", Namespace: \"suitcaselab-default\"\nfor: \"STDIN\": Pod \"recorder-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-6sljp\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-6sljp\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 3 identical fields\n  \t\t\tArgs:       nil,\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts: []core.ContainerPort{\n  \t\t\t\t{\n- \t\t\t\t\tName:          \"recorder\",\n+ \t\t\t\t\tName:          \"http-server\",\n  \t\t\t\t\tHostPort:      0,\n  \t\t\t\t\tContainerPort: 20777,\n  \t\t\t\t\t... // 2 identical fields\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tEnvFrom: nil,\n  \t\t\tEnv:     nil,\n  \t\t\t... // 14 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolume\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052\\\",\\\"pv.beta.kubernetes.io/gid\\\":\\\"1234\\\",\\\"service\\\":\\\"recorder\\\"},\\\"labels\\\":{\\\"service\\\":\\\"recorder\\\",\\\"type\\\":\\\"local\\\"},\\\"name\\\":\\\"data-pv-volume\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"capacity\\\":{\\\"storage\\\":\\\"10Gi\\\"},\\\"hostPath\\\":{\\\"path\\\":\\\"/mnt/disks/data\\\"},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\"}},\"spec\":{\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumes\", GroupVersionKind: \"/v1, Kind=PersistentVolume\"\nName: \"data-pv-volume\", Namespace: \"\"\nfor: \"STDIN\": PersistentVolume \"data-pv-volume\" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation\n  core.PersistentVolumeSource{\n  \tGCEPersistentDisk:    nil,\n  \tAWSElasticBlockStore: nil,\n  \tHostPath: &core.HostPathVolumeSource{\n- \t\tPath: \"/mnt/disks/data\",\n+ \t\tPath: \"/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data\",\n  \t\tType: &\"\",\n  \t},\n  \tGlusterfs: nil,\n  \tNFS:       nil,\n  \t... // 17 identical fields\n  }\n\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolumeClaim\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca\\\",\\\"service\\\":\\\"recorder\\\"},\\\"labels\\\":{\\\"service\\\":\\\"recorder\\\"},\\\"name\\\":\\\"data-pv-claim\\\",\\\"namespace\\\":\\\"suitcaselab-default\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"storage\\\":\\\"5Gi\\\"}},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\"}},\"spec\":{\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumeclaims\", GroupVersionKind: \"/v1, Kind=PersistentVolumeClaim\"\nName: \"data-pv-claim\", Namespace: \"suitcaselab-default\"\nfor: \"STDIN\": PersistentVolumeClaim \"data-pv-claim\" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims\n  core.PersistentVolumeClaimSpec{\n  \t... // 2 identical fields\n  \tResources:        core.ResourceRequirements{Requests: core.ResourceList{s\"storage\": {i: resource.int64Amount{value: 3221225472}, s: \"3Gi\", Format: \"BinarySI\"}}},\n  \tVolumeName:       \"data-pv-volume\",\n- \tStorageClassName: &\"standard\",\n+ \tStorageClassName: &\"manual\",\n  \tVolumeMode:       &\"Filesystem\",\n  \tDataSource:       nil,\n  }\n"
all: "Error from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Pod\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"19a7fd9726fbcc0d1092fc6f9c368fe8f0f3db51b673765b303c7fb317f5c8ae\\\",\\\"service\\\":\\\"recorder\\\"},\\\"labels\\\":{\\\"app\\\":\\\"f1-recorder\\\",\\\"service\\\":\\\"recorder\\\"},\\\"name\\\":\\\"recorder-pod\\\",\\\"namespace\\\":\\\"suitcaselab-default\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"image\\\":\\\"mgunter/python-f1-record\\\",\\\"name\\\":\\\"f1-recorder\\\",\\\"ports\\\":[{\\\"containerPort\\\":20777,\\\"name\\\":\\\"recorder\\\"}],\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/sensable\\\",\\\"name\\\":\\\"data-pv-storage\\\"}]}],\\\"volumes\\\":[{\\\"name\\\":\\\"data-pv-storage\\\",\\\"persistentVolumeClaim\\\":{\\\"claimName\\\":\\\"data-pv-claim\\\"}}]}}\\n\"}},\"spec\":{\"$setElementOrder/containers\":[{\"name\":\"f1-recorder\"}],\"$setElementOrder/volumes\":[{\"name\":\"data-pv-storage\"}],\"containers\":[{\"$setElementOrder/ports\":[{\"containerPort\":20777}],\"$setElementOrder/volumeMounts\":[{\"mountPath\":\"/sensable\"}],\"name\":\"f1-recorder\",\"ports\":[{\"containerPort\":20777,\"name\":\"recorder\"}]}]}}\nto:\nResource: \"/v1, Resource=pods\", GroupVersionKind: \"/v1, Kind=Pod\"\nName: \"recorder-pod\", Namespace: \"suitcaselab-default\"\nfor: \"STDIN\": Pod \"recorder-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-6sljp\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-6sljp\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 3 identical fields\n  \t\t\tArgs:       nil,\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts: []core.ContainerPort{\n  \t\t\t\t{\n- \t\t\t\t\tName:          \"recorder\",\n+ \t\t\t\t\tName:          \"http-server\",\n  \t\t\t\t\tHostPort:      0,\n  \t\t\t\t\tContainerPort: 20777,\n  \t\t\t\t\t... // 2 identical fields\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tEnvFrom: nil,\n  \t\t\tEnv:     nil,\n  \t\t\t... // 14 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolume\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"3b0f27d24e38de4e3aaf3c1e21b3b777f475c861185a1138c58e3ab432ece052\\\",\\\"pv.beta.kubernetes.io/gid\\\":\\\"1234\\\",\\\"service\\\":\\\"recorder\\\"},\\\"labels\\\":{\\\"service\\\":\\\"recorder\\\",\\\"type\\\":\\\"local\\\"},\\\"name\\\":\\\"data-pv-volume\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"capacity\\\":{\\\"storage\\\":\\\"10Gi\\\"},\\\"hostPath\\\":{\\\"path\\\":\\\"/mnt/disks/data\\\"},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\"}},\"spec\":{\"hostPath\":{\"path\":\"/mnt/disks/data\"},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumes\", GroupVersionKind: \"/v1, Kind=PersistentVolume\"\nName: \"data-pv-volume\", Namespace: \"\"\nfor: \"STDIN\": PersistentVolume \"data-pv-volume\" is invalid: spec.persistentvolumesource: Forbidden: spec.persistentvolumesource is immutable after creation\n  core.PersistentVolumeSource{\n  \tGCEPersistentDisk:    nil,\n  \tAWSElasticBlockStore: nil,\n  \tHostPath: &core.HostPathVolumeSource{\n- \t\tPath: \"/mnt/disks/data\",\n+ \t\tPath: \"/Volumes/Macintosh HD/Users/mattgunter/Projects/telescopic-test-demo/data\",\n  \t\tType: &\"\",\n  \t},\n  \tGlusterfs: nil,\n  \tNFS:       nil,\n  \t... // 17 identical fields\n  }\n\nError from server (Invalid): error when applying patch:\n{\"metadata\":{\"annotations\":{\"garden.io/manifest-hash\":\"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"PersistentVolumeClaim\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"garden.io/manifest-hash\\\":\\\"454f27e326cec7a71a045e860948a41a08545b6c9e862174dd47e4b12fc560ca\\\",\\\"service\\\":\\\"recorder\\\"},\\\"labels\\\":{\\\"service\\\":\\\"recorder\\\"},\\\"name\\\":\\\"data-pv-claim\\\",\\\"namespace\\\":\\\"suitcaselab-default\\\"},\\\"spec\\\":{\\\"accessModes\\\":[\\\"ReadWriteOnce\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"storage\\\":\\\"5Gi\\\"}},\\\"storageClassName\\\":\\\"standard\\\"}}\\n\",\"service\":\"recorder\"},\"labels\":{\"service\":\"recorder\"}},\"spec\":{\"resources\":{\"requests\":{\"storage\":\"5Gi\"}},\"storageClassName\":\"standard\"}}\nto:\nResource: \"/v1, Resource=persistentvolumeclaims\", GroupVersionKind: \"/v1, Kind=PersistentVolumeClaim\"\nName: \"data-pv-claim\", Namespace: \"suitcaselab-default\"\nfor: \"STDIN\": PersistentVolumeClaim \"data-pv-claim\" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests for bound claims\n  core.PersistentVolumeClaimSpec{\n  \t... // 2 identical fields\n  \tResources:        core.ResourceRequirements{Requests: core.ResourceList{s\"storage\": {i: resource.int64Amount{value: 3221225472}, s: \"3Gi\", Format: \"BinarySI\"}}},\n  \tVolumeName:       \"data-pv-volume\",\n- \tStorageClassName: &\"standard\",\n+ \tStorageClassName: &\"manual\",\n  \tVolumeMode:       &\"Filesystem\",\n  \tDataSource:       nil,\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T19:34:00.149Z] 2 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T19:33:59.289Z
    completedAt: 2021-04-14T19:34:00.038Z
    batchId: a5351aa3-e615-4fd1-83cf-a21dbd96a8d6
    version: v-0b70033d08
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-14T19:33:59.330Z
    completedAt: 2021-04-14T19:34:00.084Z
    batchId: cd5d1f00-cd5a-4897-a8e2-39dc9272a13d
    version: v-06af44e393


[2021-04-14T19:52:08.251Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=recorder --output=json -f -" failed with code 1:

The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid value: 30002: provided port is already allocated

The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid value: 30002: provided port is already allocated

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30002: provided port is already allocated
all: >-
  The Service "udp-recorder-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30002: provided port is already allocated
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T19:52:08.260Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-14T19:52:07.988Z
    completedAt: 2021-04-14T19:52:08.247Z
    batchId: ac0cdecb-2aa9-497c-bc4a-9dd29b10ec41
    version: v-ec86815564


[2021-04-14T19:55:43.134Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid value: 30001: provided port is already allocated

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
all: >-
  The Service "udp-player-service" is invalid: spec.ports[0].nodePort: Invalid
  value: 30001: provided port is already allocated
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T19:55:43.147Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T19:55:42.886Z
    completedAt: 2021-04-14T19:55:43.128Z
    batchId: 2c1cd233-8a8f-4bb3-8f3c-1d35900b53ea
    version: v-3dce37b395


[2021-04-14T20:02:41.021Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-285gp", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-285gp", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			Name:       "f1-player",
			Image:      "mgunter/python-f1-play",
			Command:    nil,
- 			Args:       []string{"test2.sqlite3"},
+ 			Args:       []string{"test1.sqlite3"},
			WorkingDir: "",
			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "TCP"}},
			EnvFrom:    nil,
			Env:        nil,
			Resources:  core.ResourceRequirements{},
			VolumeMounts: []core.VolumeMount{
				{
- 					Name:             "default-token-285gp",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "/sensable",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-285gp",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "  /sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
			},
			VolumeDevices: nil,
			LivenessProbe: nil,
			... // 10 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-285gp", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-285gp", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test2.sqlite3"},
+ 			Args:       []string{"test1.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "TCP"}},
  			EnvFrom:    nil,
  			Env:        nil,
  			Resources:  core.ResourceRequirements{},
  			VolumeMounts: []core.VolumeMount{
  				{
- 					Name:             "default-token-285gp",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "/sensable",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-285gp",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "  /sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  			},
  			VolumeDevices: nil,
  			LivenessProbe: nil,
  			... // 10 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-285gp\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-285gp\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test2.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test1.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"TCP\"}},\n  \t\t\tEnvFrom:    nil,\n  \t\t\tEnv:        nil,\n  \t\t\tResources:  core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-285gp\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"/sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-285gp\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"  /sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-285gp\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-285gp\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test2.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test1.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"TCP\"}},\n  \t\t\tEnvFrom:    nil,\n  \t\t\tEnv:        nil,\n  \t\t\tResources:  core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-285gp\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"/sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-285gp\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"  /sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:02:44.286Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T20:02:40.747Z
    completedAt: 2021-04-14T20:02:41.005Z
    batchId: 32d8d81b-770a-4b20-a694-c6b57d609edb
    version: v-00950b8c28


[2021-04-14T20:06:43.933Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			... // 7 identical fields
			Env:       nil,
			Resources: core.ResourceRequirements{},
			VolumeMounts: []core.VolumeMount{
				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
			},
			VolumeDevices: nil,
			LivenessProbe: nil,
			... // 10 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 7 identical fields
  			Env:       nil,
  			Resources: core.ResourceRequirements{},
  			VolumeMounts: []core.VolumeMount{
  				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  			},
  			VolumeDevices: nil,
  			LivenessProbe: nil,
  			... // 10 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 7 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 7 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:06:43.947Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T20:06:43.687Z
    completedAt: 2021-04-14T20:06:43.920Z
    batchId: 6825bf53-239a-4a58-bfec-c622c35f81bb
    version: v-9d03cd1d8e


[2021-04-14T20:07:20.950Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			... // 7 identical fields
			Env:       nil,
			Resources: core.ResourceRequirements{},
			VolumeMounts: []core.VolumeMount{
				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
			},
			VolumeDevices: nil,
			LivenessProbe: nil,
			... // 10 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 7 identical fields
  			Env:       nil,
  			Resources: core.ResourceRequirements{},
  			VolumeMounts: []core.VolumeMount{
  				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  			},
  			VolumeDevices: nil,
  			LivenessProbe: nil,
  			... // 10 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 7 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 7 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:07:38.027Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			... // 7 identical fields
			Env:       nil,
			Resources: core.ResourceRequirements{},
			VolumeMounts: []core.VolumeMount{
				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
			},
			VolumeDevices: nil,
			LivenessProbe: nil,
			... // 10 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 7 identical fields
  			Env:       nil,
  			Resources: core.ResourceRequirements{},
  			VolumeMounts: []core.VolumeMount{
  				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  			},
  			VolumeDevices: nil,
  			LivenessProbe: nil,
  			... // 10 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 7 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 7 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:08:06.056Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			Name:       "f1-player",
			Image:      "mgunter/python-f1-play",
			Command:    nil,
- 			Args:       []string{"test1.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
			WorkingDir: "",
			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "TCP"}},
			EnvFrom:    nil,
			Env:        nil,
			Resources:  core.ResourceRequirements{},
			VolumeMounts: []core.VolumeMount{
				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
			},
			VolumeDevices: nil,
			LivenessProbe: nil,
			... // 10 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test1.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "TCP"}},
  			EnvFrom:    nil,
  			Env:        nil,
  			Resources:  core.ResourceRequirements{},
  			VolumeMounts: []core.VolumeMount{
  				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  			},
  			VolumeDevices: nil,
  			LivenessProbe: nil,
  			... // 10 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test1.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test2.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"TCP\"}},\n  \t\t\tEnvFrom:    nil,\n  \t\t\tEnv:        nil,\n  \t\t\tResources:  core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test1.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test2.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"TCP\"}},\n  \t\t\tEnvFrom:    nil,\n  \t\t\tEnv:        nil,\n  \t\t\tResources:  core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:08:17.077Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			Name:       "f1-player",
			Image:      "mgunter/python-f1-play",
			Command:    nil,
- 			Args:       []string{"test1.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
			WorkingDir: "",
			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "TCP"}},
			EnvFrom:    nil,
			Env:        nil,
			Resources:  core.ResourceRequirements{},
			VolumeMounts: []core.VolumeMount{
				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
					SubPath:          "",
					MountPropagation: nil,
					SubPathExpr:      "",
				},
			},
			VolumeDevices: nil,
			LivenessProbe: nil,
			... // 10 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9zl22", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9zl22", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test1.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "TCP"}},
  			EnvFrom:    nil,
  			Env:        nil,
  			Resources:  core.ResourceRequirements{},
  			VolumeMounts: []core.VolumeMount{
  				{
- 					Name:             "default-token-9zl22",
+ 					Name:             "data-pv-storage",
- 					ReadOnly:         true,
+ 					ReadOnly:         false,
- 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
+ 					MountPath:        "  /sensable",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  				{
- 					Name:             "data-pv-storage",
+ 					Name:             "default-token-9zl22",
- 					ReadOnly:         false,
+ 					ReadOnly:         true,
- 					MountPath:        "/sensable",
+ 					MountPath:        "/var/run/secrets/kubernetes.io/serviceaccount",
  					SubPath:          "",
  					MountPropagation: nil,
  					SubPathExpr:      "",
  				},
  			},
  			VolumeDevices: nil,
  			LivenessProbe: nil,
  			... // 10 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test1.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test2.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"TCP\"}},\n  \t\t\tEnvFrom:    nil,\n  \t\t\tEnv:        nil,\n  \t\t\tResources:  core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9zl22\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9zl22\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test1.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test2.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"TCP\"}},\n  \t\t\tEnvFrom:    nil,\n  \t\t\tEnv:        nil,\n  \t\t\tResources:  core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"default-token-9zl22\",\n+ \t\t\t\t\tName:             \"data-pv-storage\",\n- \t\t\t\t\tReadOnly:         true,\n+ \t\t\t\t\tReadOnly:         false,\n- \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t\tMountPath:        \"  /sensable\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t\t{\n- \t\t\t\t\tName:             \"data-pv-storage\",\n+ \t\t\t\t\tName:             \"default-token-9zl22\",\n- \t\t\t\t\tReadOnly:         false,\n+ \t\t\t\t\tReadOnly:         true,\n- \t\t\t\t\tMountPath:        \"/sensable\",\n+ \t\t\t\t\tMountPath:        \"/var/run/secrets/kubernetes.io/serviceaccount\",\n  \t\t\t\t\tSubPath:          \"\",\n  \t\t\t\t\tMountPropagation: nil,\n  \t\t\t\t\tSubPathExpr:      \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:08:20.411Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T20:08:16.812Z
    completedAt: 2021-04-14T20:08:17.060Z
    batchId: 29de34a5-3589-408b-afd5-478443215266
    version: v-bf0b7dcbb4


[2021-04-14T20:17:58.601Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-d7h7b", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-d7h7b", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			... // 3 identical fields
			Args:       []string{"test1.sqlite3"},
			WorkingDir: "",
			Ports: []core.ContainerPort{
				{
					Name:          "udp-server",
					HostPort:      0,
					ContainerPort: 20777,
- 					Protocol:      "UDP",
+ 					Protocol:      "TCP",
					HostIP:        "",
				},
			},
			EnvFrom: nil,
			Env:     nil,
			... // 14 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-d7h7b", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-d7h7b", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 3 identical fields
  			Args:       []string{"test1.sqlite3"},
  			WorkingDir: "",
  			Ports: []core.ContainerPort{
  				{
  					Name:          "udp-server",
  					HostPort:      0,
  					ContainerPort: 20777,
- 					Protocol:      "UDP",
+ 					Protocol:      "TCP",
  					HostIP:        "",
  				},
  			},
  			EnvFrom: nil,
  			Env:     nil,
  			... // 14 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-d7h7b\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-d7h7b\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 3 identical fields\n  \t\t\tArgs:       []string{\"test1.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts: []core.ContainerPort{\n  \t\t\t\t{\n  \t\t\t\t\tName:          \"udp-server\",\n  \t\t\t\t\tHostPort:      0,\n  \t\t\t\t\tContainerPort: 20777,\n- \t\t\t\t\tProtocol:      \"UDP\",\n+ \t\t\t\t\tProtocol:      \"TCP\",\n  \t\t\t\t\tHostIP:        \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tEnvFrom: nil,\n  \t\t\tEnv:     nil,\n  \t\t\t... // 14 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-d7h7b\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-d7h7b\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 3 identical fields\n  \t\t\tArgs:       []string{\"test1.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts: []core.ContainerPort{\n  \t\t\t\t{\n  \t\t\t\t\tName:          \"udp-server\",\n  \t\t\t\t\tHostPort:      0,\n  \t\t\t\t\tContainerPort: 20777,\n- \t\t\t\t\tProtocol:      \"UDP\",\n+ \t\t\t\t\tProtocol:      \"TCP\",\n  \t\t\t\t\tHostIP:        \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tEnvFrom: nil,\n  \t\t\tEnv:     nil,\n  \t\t\t... // 14 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:17:58.621Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=recorder --output=json -f -" failed with code 1:

  The Pod "recorder-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-d7h7b", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-d7h7b", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			... // 3 identical fields
			Args:       nil,
			WorkingDir: "",
			Ports: []core.ContainerPort{
				{
					Name:          "recorder",
					HostPort:      0,
					ContainerPort: 20777,
- 					Protocol:      "UDP",
+ 					Protocol:      "TCP",
					HostIP:        "",
				},
			},
			EnvFrom: nil,
			Env:     nil,
			... // 14 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "Always",
	... // 25 identical fields
}

The Pod "recorder-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-d7h7b", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-d7h7b", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 3 identical fields
  			Args:       nil,
  			WorkingDir: "",
  			Ports: []core.ContainerPort{
  				{
  					Name:          "recorder",
  					HostPort:      0,
  					ContainerPort: 20777,
- 					Protocol:      "UDP",
+ 					Protocol:      "TCP",
  					HostIP:        "",
  				},
  			},
  			EnvFrom: nil,
  			Env:     nil,
  			... // 14 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "Always",
  	... // 25 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=recorder
  --output=json -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"recorder-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-d7h7b\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-d7h7b\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 3 identical fields\n  \t\t\tArgs:       nil,\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts: []core.ContainerPort{\n  \t\t\t\t{\n  \t\t\t\t\tName:          \"recorder\",\n  \t\t\t\t\tHostPort:      0,\n  \t\t\t\t\tContainerPort: 20777,\n- \t\t\t\t\tProtocol:      \"UDP\",\n+ \t\t\t\t\tProtocol:      \"TCP\",\n  \t\t\t\t\tHostIP:        \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tEnvFrom: nil,\n  \t\t\tEnv:     nil,\n  \t\t\t... // 14 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
all: "The Pod \"recorder-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-d7h7b\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-d7h7b\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\t... // 3 identical fields\n  \t\t\tArgs:       nil,\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts: []core.ContainerPort{\n  \t\t\t\t{\n  \t\t\t\t\tName:          \"recorder\",\n  \t\t\t\t\tHostPort:      0,\n  \t\t\t\t\tContainerPort: 20777,\n- \t\t\t\t\tProtocol:      \"UDP\",\n+ \t\t\t\t\tProtocol:      \"TCP\",\n  \t\t\t\t\tHostIP:        \"\",\n  \t\t\t\t},\n  \t\t\t},\n  \t\t\tEnvFrom: nil,\n  \t\t\tEnv:     nil,\n  \t\t\t... // 14 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"Always\",\n  \t... // 25 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-14T20:17:58.633Z] 2 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-14T20:17:58.053Z
    completedAt: 2021-04-14T20:17:58.589Z
    batchId: 4e77c755-1d58-4348-8ba7-0889ecbdb743
    version: v-fe8cb364b1
  deploy.recorder:
    type: deploy
    description: deploying service 'recorder' (from module 'recorder')
    key: deploy.recorder
    name: recorder
    startedAt: 2021-04-14T20:17:58.007Z
    completedAt: 2021-04-14T20:17:58.611Z
    batchId: 39b0ec6f-ee01-48f3-ada8-c30d0aef77f6
    version: v-6057922dec


[2021-04-14T21:51:41.603Z] Failed resolving one or more modules:

player: Error validating Module 'player': key "Disabled" is not allowed at path [Disabled]
recorder: Error validating Module 'recorder': key "Disabled" is not allowed at path [Disabled]

Error Details:

errors: {}


[2021-04-15T00:55:46.363Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-ch6sw", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-ch6sw", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 3 identical fields
  			Args:       []string{"test1.sqlite3"},
  			WorkingDir: "",
  			Ports: []core.ContainerPort{
  				{
  					Name:          "udp-server",
  					HostPort:      0,
  					ContainerPort: 20777,
- 					Protocol:      "UDP",
+ 					Protocol:      "TCP",
  					HostIP:        "",
  				},
  			},
  			EnvFrom: nil,
  			Env:     nil,
  			... // 14 identical fields
  		},
  	},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-ch6sw", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-ch6sw", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			... // 3 identical fields
  			Args:       []string{"test1.sqlite3"},
  			WorkingDir: "",
  			Ports: []core.ContainerPort{
  				{
  					Name:          "udp-server",
  					HostPort:      0,
  					ContainerPort: 20777,
- 					Protocol:      "UDP",
+ 					Protocol:      "TCP",
  					HostIP:        "",
  				},
  			},
  			EnvFrom: nil,
  			Env:     nil,
  			... // 14 identical fields
  		},
  	},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n\_\_core.PodSpec{\n\_\_\tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-ch6sw\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-ch6sw\", DefaultMode: &420}}}},\n\_\_\tInitContainers: nil,\n\_\_\tContainers: []core.Container{\n\_\_\t\t{\n\_\_\t\t\t... // 3 identical fields\n\_\_\t\t\tArgs:       []string{\"test1.sqlite3\"},\n\_\_\t\t\tWorkingDir: \"\",\n\_\_\t\t\tPorts: []core.ContainerPort{\n\_\_\t\t\t\t{\n\_\_\t\t\t\t\tName:          \"udp-server\",\n\_\_\t\t\t\t\tHostPort:      0,\n\_\_\t\t\t\t\tContainerPort: 20777,\n-\_\t\t\t\t\tProtocol:      \"UDP\",\n+\_\t\t\t\t\tProtocol:      \"TCP\",\n\_\_\t\t\t\t\tHostIP:        \"\",\n\_\_\t\t\t\t},\n\_\_\t\t\t},\n\_\_\t\t\tEnvFrom: nil,\n\_\_\t\t\tEnv:     nil,\n\_\_\t\t\t... // 14 identical fields\n\_\_\t\t},\n\_\_\t},\n\_\_\tEphemeralContainers:           nil,\n-\_\tRestartPolicy:                 \"OnFailure\",\n+\_\tRestartPolicy:                 \"Always\",\n\_\_\tTerminationGracePeriodSeconds: &30,\n\_\_\tActiveDeadlineSeconds:         nil,\n\_\_\t... // 22 identical fields\n\_\_}\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n\_\_core.PodSpec{\n\_\_\tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-ch6sw\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-ch6sw\", DefaultMode: &420}}}},\n\_\_\tInitContainers: nil,\n\_\_\tContainers: []core.Container{\n\_\_\t\t{\n\_\_\t\t\t... // 3 identical fields\n\_\_\t\t\tArgs:       []string{\"test1.sqlite3\"},\n\_\_\t\t\tWorkingDir: \"\",\n\_\_\t\t\tPorts: []core.ContainerPort{\n\_\_\t\t\t\t{\n\_\_\t\t\t\t\tName:          \"udp-server\",\n\_\_\t\t\t\t\tHostPort:      0,\n\_\_\t\t\t\t\tContainerPort: 20777,\n-\_\t\t\t\t\tProtocol:      \"UDP\",\n+\_\t\t\t\t\tProtocol:      \"TCP\",\n\_\_\t\t\t\t\tHostIP:        \"\",\n\_\_\t\t\t\t},\n\_\_\t\t\t},\n\_\_\t\t\tEnvFrom: nil,\n\_\_\t\t\tEnv:     nil,\n\_\_\t\t\t... // 14 identical fields\n\_\_\t\t},\n\_\_\t},\n\_\_\tEphemeralContainers:           nil,\n-\_\tRestartPolicy:                 \"OnFailure\",\n+\_\tRestartPolicy:                 \"Always\",\n\_\_\tTerminationGracePeriodSeconds: &30,\n\_\_\tActiveDeadlineSeconds:         nil,\n\_\_\t... // 22 identical fields\n\_\_}\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-15T00:55:46.396Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-15T00:55:41.138Z
    completedAt: 2021-04-15T00:55:46.320Z
    batchId: 862f2245-ff1f-4e5f-9942-51d4ab90606d
    version: v-d496daffa6


[2021-04-15T01:21:16.901Z] Timed out waiting for player to deploy

Error Details:

statuses: []


[2021-04-15T01:21:16.915Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-15T01:15:56.096Z
    completedAt: 2021-04-15T01:21:16.881Z
    batchId: 253e9292-d0e2-4794-85e9-7e40be26b32b
    version: v-d496daffa6


[2021-04-15T01:34:08.631Z] Timed out waiting for player to deploy

Error Details:

statuses: []


[2021-04-15T01:34:08.641Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-15T01:28:49.079Z
    completedAt: 2021-04-15T01:34:08.615Z
    batchId: e8a8979f-21ef-48bd-84e9-21483926b557
    version: v-d496daffa6


[2021-04-27T01:16:51.794Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=minikube apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-45b2r", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-45b2r", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
			Name:       "f1-player",
			Image:      "mgunter/python-f1-play",
			Command:    nil,
- 			Args:       []string{"test2.sqlite3"},
+ 			Args:       []string{"test1.sqlite3"},
			WorkingDir: "",
			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}},
			... // 16 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "OnFailure",
	... // 24 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-45b2r", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-45b2r", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test2.sqlite3"},
+ 			Args:       []string{"test1.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}},
  			... // 16 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "OnFailure",
  	... // 24 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-45b2r\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-45b2r\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test2.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test1.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}},\n  \t\t\t... // 16 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"OnFailure\",\n  \t... // 24 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-45b2r\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-45b2r\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n  \t\t\tName:       \"f1-player\",\n  \t\t\tImage:      \"mgunter/python-f1-play\",\n  \t\t\tCommand:    nil,\n- \t\t\tArgs:       []string{\"test2.sqlite3\"},\n+ \t\t\tArgs:       []string{\"test1.sqlite3\"},\n  \t\t\tWorkingDir: \"\",\n  \t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}},\n  \t\t\t... // 16 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"OnFailure\",\n  \t... // 24 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-27T01:16:51.807Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-27T01:16:51.242Z
    completedAt: 2021-04-27T01:16:51.781Z
    batchId: 357a2f72-d0a5-4edc-a675-b1598a06100b
    version: v-6728495926


[2021-04-27T02:37:51.761Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=minikube apply --prune --selector service=local-pv --output=json -f -" failed with code 1:

The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is immutable after creation except resources.requests for bound claims

The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is immutable after creation except resources.requests for bound claims

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=local-pv --output=json -f
  -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=local-pv --output=json -f
  -
exitCode: 1
stdout: ''
stderr: >-
  The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is
  immutable after creation except resources.requests for bound claims
all: >-
  The PersistentVolumeClaim "data-pv-claim" is invalid: spec: Forbidden: is
  immutable after creation except resources.requests for bound claims
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-27T02:37:51.770Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.local-pv:
    type: deploy
    description: deploying service 'local-pv' (from module 'local-pv')
    key: deploy.local-pv
    name: local-pv
    startedAt: 2021-04-27T02:37:51.500Z
    completedAt: 2021-04-27T02:37:51.756Z
    batchId: a01a00c7-c318-4580-9a0d-dc8bfe524fda
    version: v-07aceef360


[2021-04-27T14:23:18.298Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=minikube apply --prune --selector service=local-pv --output=json -f -" failed with code 1:

The StorageClass "standard" is invalid: reclaimPolicy: Forbidden: updates to reclaimPolicy are forbidden.

The StorageClass "standard" is invalid: reclaimPolicy: Forbidden: updates to reclaimPolicy are forbidden.

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=local-pv --output=json -f
  -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=local-pv --output=json -f
  -
exitCode: 1
stdout: ''
stderr: >-
  The StorageClass "standard" is invalid: reclaimPolicy: Forbidden: updates to
  reclaimPolicy are forbidden.
all: >-
  The StorageClass "standard" is invalid: reclaimPolicy: Forbidden: updates to
  reclaimPolicy are forbidden.
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-27T14:23:18.303Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.local-pv:
    type: deploy
    description: deploying service 'local-pv' (from module 'local-pv')
    key: deploy.local-pv
    name: local-pv
    startedAt: 2021-04-27T14:23:17.932Z
    completedAt: 2021-04-27T14:23:18.294Z
    batchId: 43e9d524-4803-4414-972b-b783c02bbe07
    version: v-07aceef360


[2021-04-27T14:42:19.368Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=minikube apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9qbr4", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9qbr4", DefaultMode: &420}}}},
	InitContainers: nil,
	Containers: []core.Container{
		{
- 			Name:    "f1-player-image",
+ 			Name:    "f1-player",
			Image:   "mgunter/python-f1-play",
			Command: nil,
			... // 4 identical fields
			Env:       nil,
			Resources: core.ResourceRequirements{},
			VolumeMounts: []core.VolumeMount{
				{Name: "data-pv-storage", MountPath: "/sensable"},
+ 				{
+ 					Name:      "default-token-9qbr4",
+ 					ReadOnly:  true,
+ 					MountPath: "/var/run/secrets/kubernetes.io/serviceaccount",
+ 				},
			},
			VolumeDevices: nil,
			LivenessProbe: nil,
			... // 10 identical fields
		},
	},
	EphemeralContainers: nil,
	RestartPolicy:       "OnFailure",
	... // 24 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-9qbr4", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-9qbr4", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
- 			Name:    "f1-player-image",
+ 			Name:    "f1-player",
  			Image:   "mgunter/python-f1-play",
  			Command: nil,
  			... // 4 identical fields
  			Env:       nil,
  			Resources: core.ResourceRequirements{},
  			VolumeMounts: []core.VolumeMount{
  				{Name: "data-pv-storage", MountPath: "/sensable"},
+ 				{
+ 					Name:      "default-token-9qbr4",
+ 					ReadOnly:  true,
+ 					MountPath: "/var/run/secrets/kubernetes.io/serviceaccount",
+ 				},
  			},
  			VolumeDevices: nil,
  			LivenessProbe: nil,
  			... // 10 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "OnFailure",
  	... // 24 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=minikube apply --prune --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9qbr4\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9qbr4\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n- \t\t\tName:    \"f1-player-image\",\n+ \t\t\tName:    \"f1-player\",\n  \t\t\tImage:   \"mgunter/python-f1-play\",\n  \t\t\tCommand: nil,\n  \t\t\t... // 4 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{Name: \"data-pv-storage\", MountPath: \"/sensable\"},\n+ \t\t\t\t{\n+ \t\t\t\t\tName:      \"default-token-9qbr4\",\n+ \t\t\t\t\tReadOnly:  true,\n+ \t\t\t\t\tMountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"OnFailure\",\n  \t... // 24 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-9qbr4\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-9qbr4\", DefaultMode: &420}}}},\n  \tInitContainers: nil,\n  \tContainers: []core.Container{\n  \t\t{\n- \t\t\tName:    \"f1-player-image\",\n+ \t\t\tName:    \"f1-player\",\n  \t\t\tImage:   \"mgunter/python-f1-play\",\n  \t\t\tCommand: nil,\n  \t\t\t... // 4 identical fields\n  \t\t\tEnv:       nil,\n  \t\t\tResources: core.ResourceRequirements{},\n  \t\t\tVolumeMounts: []core.VolumeMount{\n  \t\t\t\t{Name: \"data-pv-storage\", MountPath: \"/sensable\"},\n+ \t\t\t\t{\n+ \t\t\t\t\tName:      \"default-token-9qbr4\",\n+ \t\t\t\t\tReadOnly:  true,\n+ \t\t\t\t\tMountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\",\n+ \t\t\t\t},\n  \t\t\t},\n  \t\t\tVolumeDevices: nil,\n  \t\t\tLivenessProbe: nil,\n  \t\t\t... // 10 identical fields\n  \t\t},\n  \t},\n  \tEphemeralContainers: nil,\n  \tRestartPolicy:       \"OnFailure\",\n  \t... // 24 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-27T14:42:19.384Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-04-27T14:42:19.137Z
    completedAt: 2021-04-27T14:42:19.354Z
    batchId: 44156f80-0e1a-4ba0-b28c-268b8ef3bcec
    version: v-2d8e7392de


[2021-04-27T17:36:28.922Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=local-pv --output=json -f -" failed with code 1:

The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: is immutable after creation

The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: is immutable after creation

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=local-pv --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=local-pv --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource:
  Forbidden: is immutable after creation
all: >-
  The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource:
  Forbidden: is immutable after creation
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-27T17:36:28.933Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.local-pv:
    type: deploy
    description: deploying service 'local-pv' (from module 'local-pv')
    key: deploy.local-pv
    name: local-pv
    startedAt: 2021-04-27T17:36:28.021Z
    completedAt: 2021-04-27T17:36:28.916Z
    batchId: 2beb7e59-cdab-44fb-8ec9-91fccf1f6512
    version: v-9322c4eebe


[2021-04-27T17:41:51.947Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=local-pv --output=json -f -" failed with code 1:

The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: is immutable after creation

The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource: Forbidden: is immutable after creation

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=local-pv --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=local-pv --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource:
  Forbidden: is immutable after creation
all: >-
  The PersistentVolume "data-pv-volume" is invalid: spec.persistentvolumesource:
  Forbidden: is immutable after creation
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-27T17:41:51.957Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.local-pv:
    type: deploy
    description: deploying service 'local-pv' (from module 'local-pv')
    key: deploy.local-pv
    name: local-pv
    startedAt: 2021-04-27T17:41:51.430Z
    completedAt: 2021-04-27T17:41:51.942Z
    batchId: f1cd552c-dc8a-430e-b1cf-42e0bfa00055
    version: v-9322c4eebe


[2021-04-28T02:31:45.803Z] Failed resolving one or more modules:

udp-app: Error validating Module 'udp-app': key "dependencies" is not allowed at path [dependencies]

Error Details:

errors: {}


[2021-04-28T02:33:58.640Z] Failed resolving one or more modules:

udp-app: Error validating Module 'udp-app': key "dependencies" is not allowed at path [dependencies]

Error Details:

errors: {}


[2021-04-28T02:34:24.686Z] Failed resolving one or more modules:

udp-app: Error validating Module 'udp-app': key "dependencies" is not allowed at path [dependencies]

Error Details:

errors: {}


[2021-04-28T02:46:59.330Z] Unable to run docker command: Command "/Users/mattgunter/.garden/tools/docker/fa0336ac33b46eb5/docker/docker push gcr.io/th-structure-flow-demo/udp-app:v-47a406fd21" failed with code 1:

unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Here's the full output:

The push refers to repository [gcr.io/th-structure-flow-demo/udp-app]
f7577e4c74b4: Preparing
fe33d2017d1d: Preparing
adbb30380c0c: Preparing
2d5e4dca5d45: Preparing
d8ea972acbb7: Preparing
15b9aeee8493: Preparing
2831d216c2dc: Preparing
5d09c2db1d76: Preparing
417cb9b79ade: Preparing
15b9aeee8493: Waiting
2831d216c2dc: Waiting
5d09c2db1d76: Waiting
417cb9b79ade: Waiting
unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Error Details:


args: []
cwd: >-
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/udp-app


[2021-04-28T02:47:24.376Z] Command "/Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace suitcaselab-default install flink-operator /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator --dry-run --namespace suitcaselab-default --output json --timeout 300s --values /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml" failed with code 1:

Error: rendered manifests contain a resource that already exists. Unable to continue with install: CustomResourceDefinition "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error: rendered manifests contain a resource that already exists. Unable to continue with install: CustomResourceDefinition "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
command: >-
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
exitCode: 1
stdout: ''
stderr: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: CustomResourceDefinition
  "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be
  imported into the current release: invalid ownership metadata; annotation
  validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
all: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: CustomResourceDefinition
  "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be
  imported into the current release: invalid ownership metadata; annotation
  validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-28T02:47:24.395Z] 2 deploy task(s) failed!

Error Details:

results:
  build.udp-app:
    type: build
    description: building udp-app
    key: build.udp-app
    name: udp-app
    startedAt: 2021-04-28T02:46:54.721Z
    completedAt: 2021-04-28T02:46:59.322Z
    batchId: 03a77708-fc0e-4b57-9a6d-e256215a9102
    version: v-47a406fd21
  deploy.flink-operator:
    type: deploy
    description: deploying service 'flink-operator' (from module 'flink-operator')
    key: deploy.flink-operator
    name: flink-operator
    startedAt: 2021-04-28T02:47:20.944Z
    completedAt: 2021-04-28T02:47:24.366Z
    batchId: 03a77708-fc0e-4b57-9a6d-e256215a9102
    version: v-9467fcc674


[2021-04-28T02:51:15.865Z] Command "/Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace suitcaselab-default install flink-operator /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator --dry-run --namespace suitcaselab-default --output json --timeout 300s --values /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml" failed with code 1:

Error: rendered manifests contain a resource that already exists. Unable to continue with install: CustomResourceDefinition "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error: rendered manifests contain a resource that already exists. Unable to continue with install: CustomResourceDefinition "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
command: >-
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
exitCode: 1
stdout: ''
stderr: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: CustomResourceDefinition
  "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be
  imported into the current release: invalid ownership metadata; annotation
  validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
all: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: CustomResourceDefinition
  "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be
  imported into the current release: invalid ownership metadata; annotation
  validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-28T02:51:43.824Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.flink-operator:
    type: deploy
    description: deploying service 'flink-operator' (from module 'flink-operator')
    key: deploy.flink-operator
    name: flink-operator
    startedAt: 2021-04-28T02:51:13.236Z
    completedAt: 2021-04-28T02:51:15.853Z
    batchId: 130696c1-7583-4554-8d7b-6bfd7f145469
    version: v-9467fcc674


[2021-04-28T02:53:50.485Z] Command "/Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace suitcaselab-default install flink-operator /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator --dry-run --namespace suitcaselab-default --output json --timeout 300s --values /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml" failed with code 1:

Error: rendered manifests contain a resource that already exists. Unable to continue with install: CustomResourceDefinition "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error: rendered manifests contain a resource that already exists. Unable to continue with install: CustomResourceDefinition "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
command: >-
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
exitCode: 1
stdout: ''
stderr: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: CustomResourceDefinition
  "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be
  imported into the current release: invalid ownership metadata; annotation
  validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
all: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: CustomResourceDefinition
  "flinkclusters.flinkoperator.k8s.io" in namespace "" exists and cannot be
  imported into the current release: invalid ownership metadata; annotation
  validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-28T02:53:50.518Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.flink-operator:
    type: deploy
    description: deploying service 'flink-operator' (from module 'flink-operator')
    key: deploy.flink-operator
    name: flink-operator
    startedAt: 2021-04-28T02:53:46.944Z
    completedAt: 2021-04-28T02:53:50.474Z
    batchId: 980bcdcf-7083-4476-8150-02e3be599b2f
    version: v-9467fcc674


[2021-04-28T02:54:33.815Z] Command "/Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace suitcaselab-default install flink-operator /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator --dry-run --namespace suitcaselab-default --output json --timeout 300s --values /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml" failed with code 1:

Error: rendered manifests contain a resource that already exists. Unable to continue with install: MutatingWebhookConfiguration "flink-operator-mutating-webhook-configuration" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error: rendered manifests contain a resource that already exists. Unable to continue with install: MutatingWebhookConfiguration "flink-operator-mutating-webhook-configuration" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
command: >-
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
exitCode: 1
stdout: ''
stderr: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: MutatingWebhookConfiguration
  "flink-operator-mutating-webhook-configuration" in namespace "" exists and
  cannot be imported into the current release: invalid ownership metadata;
  annotation validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
all: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: MutatingWebhookConfiguration
  "flink-operator-mutating-webhook-configuration" in namespace "" exists and
  cannot be imported into the current release: invalid ownership metadata;
  annotation validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-28T02:54:33.848Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.flink-operator:
    type: deploy
    description: deploying service 'flink-operator' (from module 'flink-operator')
    key: deploy.flink-operator
    name: flink-operator
    startedAt: 2021-04-28T02:54:30.192Z
    completedAt: 2021-04-28T02:54:33.806Z
    batchId: 327f2467-56e0-413c-b7b6-ba3831c9eb4d
    version: v-9467fcc674


[2021-04-28T02:55:44.799Z] Command "/Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace suitcaselab-default install flink-operator /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator --dry-run --namespace suitcaselab-default --output json --timeout 300s --values /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml" failed with code 1:

Error: rendered manifests contain a resource that already exists. Unable to continue with install: ValidatingWebhookConfiguration "flink-operator-validating-webhook-configuration" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error: rendered manifests contain a resource that already exists. Unable to continue with install: ValidatingWebhookConfiguration "flink-operator-validating-webhook-configuration" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error: key "meta.helm.sh/release-namespace" must equal "suitcaselab-default": current value is "flink-cluster-default"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
command: >-
  /Users/mattgunter/.garden/tools/helm/4d13a6c3bb98b810/darwin-amd64/helm
  --kube-context gke_th-structure-flow-demo_us-east1-b_stream-demo --namespace
  suitcaselab-default install flink-operator
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator
  --dry-run --namespace suitcaselab-default --output json --timeout 300s
  --values
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/flink-operator/helm-chart/flink-operator/garden-values.yml
exitCode: 1
stdout: ''
stderr: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: ValidatingWebhookConfiguration
  "flink-operator-validating-webhook-configuration" in namespace "" exists and
  cannot be imported into the current release: invalid ownership metadata;
  annotation validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
all: >-
  Error: rendered manifests contain a resource that already exists. Unable to
  continue with install: ValidatingWebhookConfiguration
  "flink-operator-validating-webhook-configuration" in namespace "" exists and
  cannot be imported into the current release: invalid ownership metadata;
  annotation validation error: key "meta.helm.sh/release-namespace" must equal
  "suitcaselab-default": current value is "flink-cluster-default"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-04-28T02:55:44.833Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.flink-operator:
    type: deploy
    description: deploying service 'flink-operator' (from module 'flink-operator')
    key: deploy.flink-operator
    name: flink-operator
    startedAt: 2021-04-28T02:55:41.436Z
    completedAt: 2021-04-28T02:55:44.788Z
    batchId: b7eb5cf8-cc39-492b-b5a6-4cf4b2215874
    version: v-9467fcc674


[2021-04-28T13:05:24.729Z] Unable to run docker command: Command "/Users/mattgunter/.garden/tools/docker/fa0336ac33b46eb5/docker/docker push gcr.io/th-structure-flow-demo/udp-app:v-47a406fd21" failed with code 1:

unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Here's the full output:

The push refers to repository [gcr.io/th-structure-flow-demo/udp-app]
f7577e4c74b4: Preparing
fe33d2017d1d: Preparing
adbb30380c0c: Preparing
2d5e4dca5d45: Preparing
d8ea972acbb7: Preparing
15b9aeee8493: Preparing
2831d216c2dc: Preparing
5d09c2db1d76: Preparing
417cb9b79ade: Preparing
2831d216c2dc: Waiting
5d09c2db1d76: Waiting
417cb9b79ade: Waiting
15b9aeee8493: Waiting
unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

Error Details:


args: []
cwd: >-
  /Users/mattgunter/Projects/Stream-test-demo/garden-projects/.garden/build/udp-app


[2021-04-28T13:05:24.757Z] 1 deploy task(s) failed!

Error Details:

results:
  build.udp-app:
    type: build
    description: building udp-app
    key: build.udp-app
    name: udp-app
    startedAt: 2021-04-28T13:05:15.847Z
    completedAt: 2021-04-28T13:05:24.720Z
    batchId: 0223114c-d843-49f2-83f2-43875ec284f5
    version: v-47a406fd21


[2021-05-03T20:45:53.115Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-x25bd", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-x25bd", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test3.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}},
  			... // 16 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "OnFailure",
  	... // 24 identical fields
  }

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-x25bd", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-x25bd", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test3.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}},
  			... // 16 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "OnFailure",
  	... // 24 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n\_\_core.PodSpec{\n\_\_\tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-x25bd\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-x25bd\", DefaultMode: &420}}}},\n\_\_\tInitContainers: nil,\n\_\_\tContainers: []core.Container{\n\_\_\t\t{\n\_\_\t\t\tName:       \"f1-player\",\n\_\_\t\t\tImage:      \"mgunter/python-f1-play\",\n\_\_\t\t\tCommand:    nil,\n-\_\t\t\tArgs:       []string{\"test3.sqlite3\"},\n+\_\t\t\tArgs:       []string{\"test2.sqlite3\"},\n\_\_\t\t\tWorkingDir: \"\",\n\_\_\t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}},\n\_\_\t\t\t... // 16 identical fields\n\_\_\t\t},\n\_\_\t},\n\_\_\tEphemeralContainers: nil,\n\_\_\tRestartPolicy:       \"OnFailure\",\n\_\_\t... // 24 identical fields\n\_\_}\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n\_\_core.PodSpec{\n\_\_\tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-x25bd\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-x25bd\", DefaultMode: &420}}}},\n\_\_\tInitContainers: nil,\n\_\_\tContainers: []core.Container{\n\_\_\t\t{\n\_\_\t\t\tName:       \"f1-player\",\n\_\_\t\t\tImage:      \"mgunter/python-f1-play\",\n\_\_\t\t\tCommand:    nil,\n-\_\t\t\tArgs:       []string{\"test3.sqlite3\"},\n+\_\t\t\tArgs:       []string{\"test2.sqlite3\"},\n\_\_\t\t\tWorkingDir: \"\",\n\_\_\t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}},\n\_\_\t\t\t... // 16 identical fields\n\_\_\t\t},\n\_\_\t},\n\_\_\tEphemeralContainers: nil,\n\_\_\tRestartPolicy:       \"OnFailure\",\n\_\_\t... // 24 identical fields\n\_\_}\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-03T20:45:54.014Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-05-03T20:45:52.838Z
    completedAt: 2021-05-03T20:45:53.099Z
    batchId: 03c73d9d-bf76-437d-86a1-f8fee40cb969
    version: v-68a7795a0f


[2021-05-03T20:46:30.186Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-x25bd", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-x25bd", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test3.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}},
  			... // 16 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "OnFailure",
  	... // 24 identical fields
  }

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	Volumes:        []core.Volume{{Name: "data-pv-storage", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: "data-pv-claim"}}}, {Name: "default-token-x25bd", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: "default-token-x25bd", DefaultMode: &420}}}},
  	InitContainers: nil,
  	Containers: []core.Container{
  		{
  			Name:       "f1-player",
  			Image:      "mgunter/python-f1-play",
  			Command:    nil,
- 			Args:       []string{"test3.sqlite3"},
+ 			Args:       []string{"test2.sqlite3"},
  			WorkingDir: "",
  			Ports:      []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}},
  			... // 16 identical fields
  		},
  	},
  	EphemeralContainers: nil,
  	RestartPolicy:       "OnFailure",
  	... // 24 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n\_\_core.PodSpec{\n\_\_\tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-x25bd\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-x25bd\", DefaultMode: &420}}}},\n\_\_\tInitContainers: nil,\n\_\_\tContainers: []core.Container{\n\_\_\t\t{\n\_\_\t\t\tName:       \"f1-player\",\n\_\_\t\t\tImage:      \"mgunter/python-f1-play\",\n\_\_\t\t\tCommand:    nil,\n-\_\t\t\tArgs:       []string{\"test3.sqlite3\"},\n+\_\t\t\tArgs:       []string{\"test2.sqlite3\"},\n\_\_\t\t\tWorkingDir: \"\",\n\_\_\t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}},\n\_\_\t\t\t... // 16 identical fields\n\_\_\t\t},\n\_\_\t},\n\_\_\tEphemeralContainers: nil,\n\_\_\tRestartPolicy:       \"OnFailure\",\n\_\_\t... // 24 identical fields\n\_\_}\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n\_\_core.PodSpec{\n\_\_\tVolumes:        []core.Volume{{Name: \"data-pv-storage\", VolumeSource: core.VolumeSource{PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{ClaimName: \"data-pv-claim\"}}}, {Name: \"default-token-x25bd\", VolumeSource: core.VolumeSource{Secret: &core.SecretVolumeSource{SecretName: \"default-token-x25bd\", DefaultMode: &420}}}},\n\_\_\tInitContainers: nil,\n\_\_\tContainers: []core.Container{\n\_\_\t\t{\n\_\_\t\t\tName:       \"f1-player\",\n\_\_\t\t\tImage:      \"mgunter/python-f1-play\",\n\_\_\t\t\tCommand:    nil,\n-\_\t\t\tArgs:       []string{\"test3.sqlite3\"},\n+\_\t\t\tArgs:       []string{\"test2.sqlite3\"},\n\_\_\t\t\tWorkingDir: \"\",\n\_\_\t\t\tPorts:      []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}},\n\_\_\t\t\t... // 16 identical fields\n\_\_\t\t},\n\_\_\t},\n\_\_\tEphemeralContainers: nil,\n\_\_\tRestartPolicy:       \"OnFailure\",\n\_\_\t... // 24 identical fields\n\_\_}\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-03T20:46:30.673Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-05-03T20:46:29.935Z
    completedAt: 2021-05-03T20:46:30.172Z
    batchId: 0109f97f-e4e6-40eb-bdfe-9135cfd05459
    version: v-68a7795a0f


[2021-05-04T14:34:52.785Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	... // 2 identical fields
	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
	EphemeralContainers:           nil,
- 	RestartPolicy:                 "Always",
+ 	RestartPolicy:                 "OnFailure",
	TerminationGracePeriodSeconds: &30,
	ActiveDeadlineSeconds:         nil,
	... // 22 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	... // 2 identical fields
  	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "Always",
+ 	RestartPolicy:                 "OnFailure",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"Always\",\n+ \tRestartPolicy:                 \"OnFailure\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"Always\",\n+ \tRestartPolicy:                 \"OnFailure\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T14:34:53.780Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-05-04T14:34:52.172Z
    completedAt: 2021-05-04T14:34:52.769Z
    batchId: 9d9fabd2-df6b-4487-b60b-cb7b5e91b513
    version: v-10222f9235


[2021-05-04T14:36:58.656Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	... // 2 identical fields
	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
	TerminationGracePeriodSeconds: &30,
	ActiveDeadlineSeconds:         nil,
	... // 22 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	... // 2 identical fields
  	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:16:09.298Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	... // 2 identical fields
	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
	TerminationGracePeriodSeconds: &30,
	ActiveDeadlineSeconds:         nil,
	... // 22 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	... // 2 identical fields
  	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:16:20.390Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	... // 2 identical fields
	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
	TerminationGracePeriodSeconds: &30,
	ActiveDeadlineSeconds:         nil,
	... // 22 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	... // 2 identical fields
  	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:28:43.794Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	... // 2 identical fields
	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
	TerminationGracePeriodSeconds: &30,
	ActiveDeadlineSeconds:         nil,
	... // 22 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	... // 2 identical fields
  	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:28:43.839Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-05-04T15:28:42.043Z
    completedAt: 2021-05-04T15:28:43.776Z
    batchId: d77ec84c-009b-4b7f-b2d0-f880b5c468b4
    version: v-02834fe69b


[2021-05-04T15:29:47.253Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

  The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
core.PodSpec{
	... // 2 identical fields
	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
	TerminationGracePeriodSeconds: &30,
	ActiveDeadlineSeconds:         nil,
	... // 22 identical fields
}

The Pod "player-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)
  core.PodSpec{
  	... // 2 identical fields
  	Containers:                    []core.Container{{Name: "f1-player", Image: "mgunter/python-f1-play", Args: []string{"test3.sqlite3"}, Ports: []core.ContainerPort{{Name: "udp-server", ContainerPort: 20777, Protocol: "UDP"}}, VolumeMounts: []core.VolumeMount{{Name: "data-pv-storage", MountPath: "/sensable"}, {Name: "default-token-x25bd", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", TerminationMessagePolicy: "File", ImagePullPolicy: "Always"}},
  	EphemeralContainers:           nil,
- 	RestartPolicy:                 "OnFailure",
+ 	RestartPolicy:                 "Always",
  	TerminationGracePeriodSeconds: &30,
  	ActiveDeadlineSeconds:         nil,
  	... // 22 identical fields
  }


Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
all: "The Pod \"player-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n  core.PodSpec{\n  \t... // 2 identical fields\n  \tContainers:                    []core.Container{{Name: \"f1-player\", Image: \"mgunter/python-f1-play\", Args: []string{\"test3.sqlite3\"}, Ports: []core.ContainerPort{{Name: \"udp-server\", ContainerPort: 20777, Protocol: \"UDP\"}}, VolumeMounts: []core.VolumeMount{{Name: \"data-pv-storage\", MountPath: \"/sensable\"}, {Name: \"default-token-x25bd\", ReadOnly: true, MountPath: \"/var/run/secrets/kubernetes.io/serviceaccount\"}}, TerminationMessagePath: \"/dev/termination-log\", TerminationMessagePolicy: \"File\", ImagePullPolicy: \"Always\"}},\n  \tEphemeralContainers:           nil,\n- \tRestartPolicy:                 \"OnFailure\",\n+ \tRestartPolicy:                 \"Always\",\n  \tTerminationGracePeriodSeconds: &30,\n  \tActiveDeadlineSeconds:         nil,\n  \t... // 22 identical fields\n  }\n"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:29:47.597Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.player:
    type: deploy
    description: deploying service 'player' (from module 'player')
    key: deploy.player
    name: player
    startedAt: 2021-05-04T15:29:46.837Z
    completedAt: 2021-05-04T15:29:47.240Z
    batchId: 22bf1660-05e6-4454-ba87-12366fb1f090
    version: v-02834fe69b


[2021-05-04T15:30:14.172Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop --namespace=suitcaselab-default delete --wait=true --ignore-not-found=true Namespace/flink-operator-system ServiceAccount/flink-operator ConfigMap/cert-configmap ConfigMap/webhook-configmap CustomResourceDefinition/flinkclusters.flinkoperator.k8s.io ClusterRole/flink-operator-manager-role ClusterRole/flink-operator-proxy-role ClusterRoleBinding/flink-operator-manager-rolebinding ClusterRoleBinding/flink-operator-proxy-rolebinding Role/flink-operator-leader-election-role RoleBinding/flink-operator-leader-election-rolebinding Service/flink-operator-controller-manager-metrics-service Service/flink-operator-webhook-service Deployment/flink-operator-controller-manager Job/cert-job MutatingWebhookConfiguration/flink-operator-mutating-webhook-configuration ValidatingWebhookConfiguration/flink-operator-validating-webhook-configuration" failed with code 1:

warning: deleting cluster-scoped resources, not scoped to the provided namespace
Error from server (Conflict): Operation cannot be fulfilled on namespaces "flink-operator-system": The system is ensuring all content is removed from this namespace.  Upon completion, this namespace will automatically be purged by the system.

warning: deleting cluster-scoped resources, not scoped to the provided namespace
Error from server (Conflict): Operation cannot be fulfilled on namespaces "flink-operator-system": The system is ensuring all content is removed from this namespace.  Upon completion, this namespace will automatically be purged by the system.

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop --namespace=suitcaselab-default delete --wait=true
  --ignore-not-found=true Namespace/flink-operator-system
  ServiceAccount/flink-operator ConfigMap/cert-configmap
  ConfigMap/webhook-configmap
  CustomResourceDefinition/flinkclusters.flinkoperator.k8s.io
  ClusterRole/flink-operator-manager-role ClusterRole/flink-operator-proxy-role
  ClusterRoleBinding/flink-operator-manager-rolebinding
  ClusterRoleBinding/flink-operator-proxy-rolebinding
  Role/flink-operator-leader-election-role
  RoleBinding/flink-operator-leader-election-rolebinding
  Service/flink-operator-controller-manager-metrics-service
  Service/flink-operator-webhook-service
  Deployment/flink-operator-controller-manager Job/cert-job
  MutatingWebhookConfiguration/flink-operator-mutating-webhook-configuration
  ValidatingWebhookConfiguration/flink-operator-validating-webhook-configuration
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop --namespace=suitcaselab-default delete --wait=true
  --ignore-not-found=true Namespace/flink-operator-system
  ServiceAccount/flink-operator ConfigMap/cert-configmap
  ConfigMap/webhook-configmap
  CustomResourceDefinition/flinkclusters.flinkoperator.k8s.io
  ClusterRole/flink-operator-manager-role ClusterRole/flink-operator-proxy-role
  ClusterRoleBinding/flink-operator-manager-rolebinding
  ClusterRoleBinding/flink-operator-proxy-rolebinding
  Role/flink-operator-leader-election-role
  RoleBinding/flink-operator-leader-election-rolebinding
  Service/flink-operator-controller-manager-metrics-service
  Service/flink-operator-webhook-service
  Deployment/flink-operator-controller-manager Job/cert-job
  MutatingWebhookConfiguration/flink-operator-mutating-webhook-configuration
  ValidatingWebhookConfiguration/flink-operator-validating-webhook-configuration
exitCode: 1
stdout: ''
stderr: >-
  warning: deleting cluster-scoped resources, not scoped to the provided
  namespace

  Error from server (Conflict): Operation cannot be fulfilled on namespaces
  "flink-operator-system": The system is ensuring all content is removed from
  this namespace.  Upon completion, this namespace will automatically be purged
  by the system.
all: >-
  warning: deleting cluster-scoped resources, not scoped to the provided
  namespace

  Error from server (Conflict): Operation cannot be fulfilled on namespaces
  "flink-operator-system": The system is ensuring all content is removed from
  this namespace.  Upon completion, this namespace will automatically be purged
  by the system.
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:35:10.490Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=flink-session-cluster --output=json -f -" failed with code 1:

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: dial tcp 10.98.84.139:443: connect: connection refused

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: dial tcp 10.98.84.139:443: connect: connection refused

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector
  service=flink-session-cluster --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector
  service=flink-session-cluster --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  dial tcp 10.98.84.139:443: connect: connection refused
all: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  dial tcp 10.98.84.139:443: connect: connection refused
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:36:36.021Z] kubernetes module player doesn't specify a serviceResource in its configuration. You must specify a resource in the module config in order to use certain Garden features, such as hot reloading, tasks and tests.

Error Details:

resourceSpec: {}


[2021-05-04T15:36:36.033Z] Task failed!

Error Details:

result:
  type: task
  description: running task play in module player
  key: task.play
  name: play
  startedAt: 2021-05-04T15:36:35.974Z
  completedAt: 2021-05-04T15:36:36.016Z
  batchId: 4256fd9c-dc75-48cc-aa0d-bb821035bcfb
  version: v-02834fe69b


[2021-05-04T15:43:55.917Z] Failed resolving one or more modules:

player: Error validating Module 'player': key .serviceResource.kind must be one of [Deployment, DaemonSet, StatefulSet]

Error Details:

errors: {}


[2021-05-04T15:46:12.261Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=flink-session-cluster --output=json -f -" failed with code 1:

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: x509: certificate signed by unknown authority

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: x509: certificate signed by unknown authority

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=flink-session-cluster --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=flink-session-cluster --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  x509: certificate signed by unknown authority
all: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  x509: certificate signed by unknown authority
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:54:26.611Z] kubernetes module player does not contain specified Deployment player-pod

Error Details:

resourceSpec:
  kind: Deployment
  name: player-pod
chartResourceNames: []


[2021-05-04T15:54:26.620Z] Task failed!

Error Details:

result:
  type: task
  description: running task play in module player
  key: task.play
  name: play
  startedAt: 2021-05-04T15:54:26.271Z
  completedAt: 2021-05-04T15:54:26.608Z
  batchId: a65da09b-f363-4f36-888e-2f0417027129
  version: v-3923779065


[2021-05-04T15:58:16.865Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Pod.spec): unknown field "replicas" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Pod.spec): unknown field "replicas" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Pod.spec): unknown field "replicas" in
  io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field
  "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown
  field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec):
  missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you
  choose to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Pod.spec): unknown field "replicas" in
  io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field
  "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown
  field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec):
  missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you
  choose to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:58:17.218Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Pod.spec): unknown field "replicas" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Pod.spec): unknown field "replicas" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Pod.spec): unknown field "replicas" in
  io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field
  "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown
  field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec):
  missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you
  choose to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Pod.spec): unknown field "replicas" in
  io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown field
  "selector" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec): unknown
  field "template" in io.k8s.api.core.v1.PodSpec, ValidationError(Pod.spec):
  missing required field "containers" in io.k8s.api.core.v1.PodSpec]; if you
  choose to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:58:32.056Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:58:32.129Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:59:08.509Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:59:09.201Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T15:59:11.588Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=flink-session-cluster --output=json -f -" failed with code 1:

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: x509: certificate signed by unknown authority

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: x509: certificate signed by unknown authority

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=flink-session-cluster --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=flink-session-cluster --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  x509: certificate signed by unknown authority
all: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  x509: certificate signed by unknown authority
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:00:15.031Z] Deployment player-pod has no containers configured.

Error Details:

resource:
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: player-pod
    labels:
      app: f1-player-deploy
      test: test2
      service: player
    namespace: suitcaselab-default
    annotations:
      service: player
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: f1-player
    template:
      metadata:
        labels:
          app: f1-player
      restartPolicy: OnFailure
      volumes: []
      containers: []


[2021-05-04T16:00:15.041Z] Task failed!

Error Details:

result:
  type: task
  description: running task play in module player
  key: task.play
  name: play
  startedAt: 2021-05-04T16:00:14.910Z
  completedAt: 2021-05-04T16:00:15.026Z
  batchId: 1119a701-fc57-4f74-ba77-a108e1180fe4
  version: v-f9a506471b


[2021-05-04T16:07:02.975Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec,
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors,
  turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:07:53.373Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:07:53.422Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:08:15.008Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:08:15.013Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:09:05.497Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:09:05.596Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "containers" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "containers" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:10:20.018Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:10:20.081Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:12:23.780Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:12:23.830Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:12:57.761Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:12:58.322Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  [ValidationError(Deployment.spec.template): unknown field "restartPolicy" in
  io.k8s.api.core.v1.PodTemplateSpec, ValidationError(Deployment.spec.template):
  unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec]; if you choose
  to ignore these errors, turn validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:13:17.848Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:13:17.865Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

error: error validating "STDIN": error validating data: ValidationError(Deployment.spec.template): unknown field "volumes" in io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn validation off with --validate=false

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
all: >-
  error: error validating "STDIN": error validating data:
  ValidationError(Deployment.spec.template): unknown field "volumes" in
  io.k8s.api.core.v1.PodTemplateSpec; if you choose to ignore these errors, turn
  validation off with --validate=false
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:14:01.473Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=player --output=json -f -" failed with code 1:

The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy: Unsupported value: "OnFailure": supported values: "Always"

The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy: Unsupported value: "OnFailure": supported values: "Always"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector service=player --output=json
  -f -
exitCode: 1
stdout: ''
stderr: >-
  The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy:
  Unsupported value: "OnFailure": supported values: "Always"
all: >-
  The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy:
  Unsupported value: "OnFailure": supported values: "Always"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:14:02.209Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=player --output=json -f -" failed with code 1:

The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy: Unsupported value: "OnFailure": supported values: "Always"

The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy: Unsupported value: "OnFailure": supported values: "Always"

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=player --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy:
  Unsupported value: "OnFailure": supported values: "Always"
all: >-
  The Deployment "player-pod" is invalid: spec.template.spec.restartPolicy:
  Unsupported value: "OnFailure": supported values: "Always"
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T16:25:03.320Z] Failed resolving one or more modules:

player: Invalid template string (${var.play}): Could not find key play under var. Available keys: record.

Error Details:

errors: {}


[2021-05-04T16:27:01.914Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:02.138Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:02.702Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:02.945Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:03.157Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:04.962Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:05.181Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:05.729Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:05.959Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:06.169Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:11.757Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:12.028Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:12.587Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:13.815Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T16:27:14.360Z] Error starting port forward to flink-operator/flink-operator-controller-manager-metrics-service:8443: Port forward exited with code 1 before establishing connection:

Error from server (NotFound): services "flink-operator-controller-manager-metrics-service" not found


[2021-05-04T18:36:54.217Z] Task play is disabled for the gke environment. If you're sure you want to run it anyway, please run the command again with the --force flag.

Error Details:

moduleName: player
taskName: play
environmentName: gke


[2021-05-04T18:44:19.682Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune --selector service=zookeeper --output=json -f -" failed with code 1:

Error from server (NotFound): error when creating "STDIN": namespaces "suitcaselab-default" not found
Error from server (NotFound): error when creating "STDIN": namespaces "suitcaselab-default" not found

Error from server (NotFound): error when creating "STDIN": namespaces "suitcaselab-default" not found
Error from server (NotFound): error when creating "STDIN": namespaces "suitcaselab-default" not found

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=zookeeper --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=gke_th-structure-flow-demo_us-east1-b_stream-demo apply --prune
  --selector service=zookeeper --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (NotFound): error when creating "STDIN": namespaces
  "suitcaselab-default" not found

  Error from server (NotFound): error when creating "STDIN": namespaces
  "suitcaselab-default" not found
all: >-
  Error from server (NotFound): error when creating "STDIN": namespaces
  "suitcaselab-default" not found

  Error from server (NotFound): error when creating "STDIN": namespaces
  "suitcaselab-default" not found
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T18:45:59.473Z] Error deploying player: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1
Pod player-pod-fdb8d4bdc-djt4p: Scheduled - Successfully assigned suitcaselab-default/player-pod-fdb8d4bdc-djt4p to gke-stream-demo-default-pool-c32f3075-yonw
Pod player-pod-fdb8d4bdc-djt4p: Pulling - Pulling image "mgunter/python-f1-play"
Pod player-pod-fdb8d4bdc-djt4p: Pulled - Successfully pulled image "mgunter/python-f1-play"
Pod player-pod-fdb8d4bdc-djt4p: Created - Created container f1-player
Pod player-pod-fdb8d4bdc-djt4p: Started - Started container f1-player
Pod player-pod-fdb8d4bdc-djt4p: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod

****** player-pod-fdb8d4bdc-djt4p ******
------ f1-player ------usage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]
                                [-p PORT]
                                filename
f1-2020-telemetry-player: error: the following arguments are required: filename


Error Details:

serviceName: player
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1\e[39m\n\e[94mPod player-pod-fdb8d4bdc-djt4p:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/player-pod-fdb8d4bdc-djt4p to gke-stream-demo-default-pool-c32f3075-yonw\e[39m\n\e[94mPod player-pod-fdb8d4bdc-djt4p:\e[39m \e[37mPulling - Pulling image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-fdb8d4bdc-djt4p:\e[39m \e[37mPulled - Successfully pulled image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-fdb8d4bdc-djt4p:\e[39m \e[37mCreated - Created container f1-player\e[39m\n\e[94mPod player-pod-fdb8d4bdc-djt4p:\e[39m \e[37mStarted - Started container f1-player\e[39m\n\e[94mPod player-pod-fdb8d4bdc-djt4p:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod\e[39m\n\e[94m\e[39m\n\e[94m****** player-pod-fdb8d4bdc-djt4p ******\e[39m\n\e[94m\e[39m\e[90m------ f1-player ------\e[39musage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]\n                                [-p PORT]\n                                filename\nf1-2020-telemetry-player: error: the following arguments are required: filename\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: player-pod
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/player-pod
      uid: 3bad6370-1b52-42be-bcce-9acbb58088fb
      resourceVersion: '10337495'
      generation: 1
      creationTimestamp: '2021-05-04T18:45:53Z'
      labels:
        app: f1-player-deploy
        service: player
        test: test2
      annotations:
        deployment.kubernetes.io/revision: '1'
        garden.io/hot-reload: 'false'
        garden.io/manifest-hash: 21d5e98e93b6caf14f0d71c296cb2fcaeafd3dbf2457cd7d42a87fd4b4ebba7a
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/hot-reload":"false","garden.io/manifest-hash":"21d5e98e93b6caf14f0d71c296cb2fcaeafd3dbf2457cd7d42a87fd4b4ebba7a","service":"player"},"labels":{"app":"f1-player-deploy","service":"player","test":"test2"},"name":"player-pod","namespace":"suitcaselab-default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"f1-player"}},"template":{"metadata":{"labels":{"app":"f1-player"}},"spec":{"containers":[{"image":"mgunter/python-f1-play","name":"f1-player","ports":[{"containerPort":20777,"name":"udp-server","protocol":"UDP"}],"volumeMounts":[{"mountPath":"/sensable","name":"data-pv-storage"}]}],"restartPolicy":"Always","volumes":[{"name":"data-pv-storage","persistentVolumeClaim":{"claimName":"data-pv-claim"}}]}}}}
        service: player
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: f1-player
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: f1-player
        spec:
          volumes: []
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 1
      replicas: 1
      updatedReplicas: 1
      unavailableReplicas: 1
      conditions: []


[2021-05-04T18:47:02.013Z] Error deploying player: BackOff - Back-off restarting failed container

━━━ Events ━━━
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1
Pod player-pod-6bfc575d89-blgtv: Scheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw
Pod player-pod-6bfc575d89-blgtv: Pulling - Pulling image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Pulled - Successfully pulled image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Created - Created container f1-player
Pod player-pod-6bfc575d89-blgtv: Started - Started container f1-player
Pod player-pod-6bfc575d89-blgtv: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod

****** player-pod-6bfc575d89-blgtv ******
------ f1-player ------usage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]
                                [-p PORT]
                                filename

Replay an F1 2019 session as UDP packets.

positional arguments:
  filename              SQLite3 file to replay packets from

optional arguments:
  -h, --help            show this help message and exit
  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR
                        playback real-time factor (higher is faster,
                        default=1.0)
  -d DESTINATION, --destination DESTINATION
                        destination UDP address; omit to use broadcast
                        (default)
  -p PORT, --port PORT  destination UDP port (default: 20777)


Error Details:

serviceName: player
status:
  state: unhealthy
  lastMessage: BackOff - Back-off restarting failed container
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulling - Pulling image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulled - Successfully pulled image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mCreated - Created container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mStarted - Started container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod\e[39m\n\e[94m\e[39m\n\e[94m****** player-pod-6bfc575d89-blgtv ******\e[39m\n\e[94m\e[39m\e[90m------ f1-player ------\e[39musage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]\n                                [-p PORT]\n                                filename\n\nReplay an F1 2019 session as UDP packets.\n\npositional arguments:\n  filename              SQLite3 file to replay packets from\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR\n                        playback real-time factor (higher is faster,\n                        default=1.0)\n  -d DESTINATION, --destination DESTINATION\n                        destination UDP address; omit to use broadcast\n                        (default)\n  -p PORT, --port PORT  destination UDP port (default: 20777)\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: player-pod
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/player-pod
      uid: 3bad6370-1b52-42be-bcce-9acbb58088fb
      resourceVersion: '10337958'
      generation: 2
      creationTimestamp: '2021-05-04T18:45:53Z'
      labels:
        app: f1-player-deploy
        service: player
        test: test2
      annotations:
        deployment.kubernetes.io/revision: '2'
        garden.io/hot-reload: 'false'
        garden.io/manifest-hash: 84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/hot-reload":"false","garden.io/manifest-hash":"84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921","service":"player"},"labels":{"app":"f1-player-deploy","service":"player","test":"test2"},"name":"player-pod","namespace":"suitcaselab-default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"f1-player"}},"template":{"metadata":{"labels":{"app":"f1-player"}},"spec":{"containers":[{"args":["-h"],"image":"mgunter/python-f1-play","name":"f1-player","ports":[{"containerPort":20777,"name":"udp-server","protocol":"UDP"}],"volumeMounts":[{"mountPath":"/sensable","name":"data-pv-storage"}]}],"restartPolicy":"Always","volumes":[{"name":"data-pv-storage","persistentVolumeClaim":{"claimName":"data-pv-claim"}}]}}}}
        service: player
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: f1-player
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: f1-player
        spec:
          volumes: []
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 2
      replicas: 2
      updatedReplicas: 1
      unavailableReplicas: 2
      conditions: []


[2021-05-04T18:51:38.107Z] Error deploying player: CrashLoopBackOff - back-off 2m40s restarting failed container=f1-player pod=player-pod-6bfc575d89-blgtv_suitcaselab-default(f28e9cb1-406e-4766-9749-1db5301a67ab)

━━━ Events ━━━
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1
Pod player-pod-6bfc575d89-blgtv: Scheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw
Pod player-pod-6bfc575d89-blgtv: Pulling - Pulling image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Pulled - Successfully pulled image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Created - Created container f1-player
Pod player-pod-6bfc575d89-blgtv: Started - Started container f1-player
Pod player-pod-6bfc575d89-blgtv: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod

****** player-pod-6bfc575d89-blgtv ******
------ f1-player ------usage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]
                                [-p PORT]
                                filename

Replay an F1 2019 session as UDP packets.

positional arguments:
  filename              SQLite3 file to replay packets from

optional arguments:
  -h, --help            show this help message and exit
  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR
                        playback real-time factor (higher is faster,
                        default=1.0)
  -d DESTINATION, --destination DESTINATION
                        destination UDP address; omit to use broadcast
                        (default)
  -p PORT, --port PORT  destination UDP port (default: 20777)


Error Details:

serviceName: player
status:
  state: unhealthy
  lastMessage: >-
    CrashLoopBackOff - back-off 2m40s restarting failed container=f1-player
    pod=player-pod-6bfc575d89-blgtv_suitcaselab-default(f28e9cb1-406e-4766-9749-1db5301a67ab)
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulling - Pulling image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulled - Successfully pulled image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mCreated - Created container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mStarted - Started container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod\e[39m\n\e[94m\e[39m\n\e[94m****** player-pod-6bfc575d89-blgtv ******\e[39m\n\e[94m\e[39m\e[90m------ f1-player ------\e[39musage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]\n                                [-p PORT]\n                                filename\n\nReplay an F1 2019 session as UDP packets.\n\npositional arguments:\n  filename              SQLite3 file to replay packets from\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR\n                        playback real-time factor (higher is faster,\n                        default=1.0)\n  -d DESTINATION, --destination DESTINATION\n                        destination UDP address; omit to use broadcast\n                        (default)\n  -p PORT, --port PORT  destination UDP port (default: 20777)\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: player-pod
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/player-pod
      uid: 3bad6370-1b52-42be-bcce-9acbb58088fb
      resourceVersion: '10337958'
      generation: 2
      creationTimestamp: '2021-05-04T18:45:53Z'
      labels:
        app: f1-player-deploy
        service: player
        test: test2
      annotations:
        deployment.kubernetes.io/revision: '2'
        garden.io/hot-reload: 'false'
        garden.io/manifest-hash: 84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/hot-reload":"false","garden.io/manifest-hash":"84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921","service":"player"},"labels":{"app":"f1-player-deploy","service":"player","test":"test2"},"name":"player-pod","namespace":"suitcaselab-default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"f1-player"}},"template":{"metadata":{"labels":{"app":"f1-player"}},"spec":{"containers":[{"args":["-h"],"image":"mgunter/python-f1-play","name":"f1-player","ports":[{"containerPort":20777,"name":"udp-server","protocol":"UDP"}],"volumeMounts":[{"mountPath":"/sensable","name":"data-pv-storage"}]}],"restartPolicy":"Always","volumes":[{"name":"data-pv-storage","persistentVolumeClaim":{"claimName":"data-pv-claim"}}]}}}}
        service: player
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: f1-player
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: f1-player
        spec:
          volumes: []
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 2
      replicas: 2
      updatedReplicas: 1
      unavailableReplicas: 2
      conditions: []


[2021-05-04T18:53:00.281Z] Error deploying player: CrashLoopBackOff - back-off 5m0s restarting failed container=f1-player pod=player-pod-6bfc575d89-blgtv_suitcaselab-default(f28e9cb1-406e-4766-9749-1db5301a67ab)

━━━ Events ━━━
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1
Pod player-pod-6bfc575d89-blgtv: Scheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw
Pod player-pod-6bfc575d89-blgtv: Pulling - Pulling image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Pulled - Successfully pulled image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Created - Created container f1-player
Pod player-pod-6bfc575d89-blgtv: Started - Started container f1-player
Pod player-pod-6bfc575d89-blgtv: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod

****** player-pod-6bfc575d89-blgtv ******
------ f1-player ------usage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]
                                [-p PORT]
                                filename

Replay an F1 2019 session as UDP packets.

positional arguments:
  filename              SQLite3 file to replay packets from

optional arguments:
  -h, --help            show this help message and exit
  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR
                        playback real-time factor (higher is faster,
                        default=1.0)
  -d DESTINATION, --destination DESTINATION
                        destination UDP address; omit to use broadcast
                        (default)
  -p PORT, --port PORT  destination UDP port (default: 20777)


Error Details:

serviceName: player
status:
  state: unhealthy
  lastMessage: >-
    CrashLoopBackOff - back-off 5m0s restarting failed container=f1-player
    pod=player-pod-6bfc575d89-blgtv_suitcaselab-default(f28e9cb1-406e-4766-9749-1db5301a67ab)
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulling - Pulling image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulled - Successfully pulled image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mCreated - Created container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mStarted - Started container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod\e[39m\n\e[94m\e[39m\n\e[94m****** player-pod-6bfc575d89-blgtv ******\e[39m\n\e[94m\e[39m\e[90m------ f1-player ------\e[39musage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]\n                                [-p PORT]\n                                filename\n\nReplay an F1 2019 session as UDP packets.\n\npositional arguments:\n  filename              SQLite3 file to replay packets from\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR\n                        playback real-time factor (higher is faster,\n                        default=1.0)\n  -d DESTINATION, --destination DESTINATION\n                        destination UDP address; omit to use broadcast\n                        (default)\n  -p PORT, --port PORT  destination UDP port (default: 20777)\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: player-pod
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/player-pod
      uid: 3bad6370-1b52-42be-bcce-9acbb58088fb
      resourceVersion: '10337958'
      generation: 2
      creationTimestamp: '2021-05-04T18:45:53Z'
      labels:
        app: f1-player-deploy
        service: player
        test: test2
      annotations:
        deployment.kubernetes.io/revision: '2'
        garden.io/hot-reload: 'false'
        garden.io/manifest-hash: 84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/hot-reload":"false","garden.io/manifest-hash":"84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921","service":"player"},"labels":{"app":"f1-player-deploy","service":"player","test":"test2"},"name":"player-pod","namespace":"suitcaselab-default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"f1-player"}},"template":{"metadata":{"labels":{"app":"f1-player"}},"spec":{"containers":[{"args":["-h"],"image":"mgunter/python-f1-play","name":"f1-player","ports":[{"containerPort":20777,"name":"udp-server","protocol":"UDP"}],"volumeMounts":[{"mountPath":"/sensable","name":"data-pv-storage"}]}],"restartPolicy":"Always","volumes":[{"name":"data-pv-storage","persistentVolumeClaim":{"claimName":"data-pv-claim"}}]}}}}
        service: player
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: f1-player
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: f1-player
        spec:
          volumes: []
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 2
      replicas: 2
      updatedReplicas: 1
      unavailableReplicas: 2
      conditions: []


[2021-05-04T18:55:05.201Z] Error deploying player: CrashLoopBackOff - back-off 5m0s restarting failed container=f1-player pod=player-pod-6bfc575d89-blgtv_suitcaselab-default(f28e9cb1-406e-4766-9749-1db5301a67ab)

━━━ Events ━━━
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1
Deployment player-pod: ScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1
Pod player-pod-6bfc575d89-blgtv: Scheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw
Pod player-pod-6bfc575d89-blgtv: Pulling - Pulling image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Pulled - Successfully pulled image "mgunter/python-f1-play"
Pod player-pod-6bfc575d89-blgtv: Created - Created container f1-player
Pod player-pod-6bfc575d89-blgtv: Started - Started container f1-player
Pod player-pod-6bfc575d89-blgtv: BackOff - Back-off restarting failed container

━━━ Pod logs ━━━
<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>
$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod

****** player-pod-6bfc575d89-blgtv ******
------ f1-player ------usage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]
                                [-p PORT]
                                filename

Replay an F1 2019 session as UDP packets.

positional arguments:
  filename              SQLite3 file to replay packets from

optional arguments:
  -h, --help            show this help message and exit
  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR
                        playback real-time factor (higher is faster,
                        default=1.0)
  -d DESTINATION, --destination DESTINATION
                        destination UDP address; omit to use broadcast
                        (default)
  -p PORT, --port PORT  destination UDP port (default: 20777)


Error Details:

serviceName: player
status:
  state: unhealthy
  lastMessage: >-
    CrashLoopBackOff - back-off 5m0s restarting failed container=f1-player
    pod=player-pod-6bfc575d89-blgtv_suitcaselab-default(f28e9cb1-406e-4766-9749-1db5301a67ab)
  logs: "\e[37m━━━ Events ━━━\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-fdb8d4bdc to 1\e[39m\n\e[94mDeployment player-pod:\e[39m \e[37mScalingReplicaSet - Scaled up replica set player-pod-6bfc575d89 to 1\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mScheduled - Successfully assigned suitcaselab-default/player-pod-6bfc575d89-blgtv to gke-stream-demo-default-pool-c32f3075-yonw\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulling - Pulling image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mPulled - Successfully pulled image \"mgunter/python-f1-play\"\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mCreated - Created container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[37mStarted - Started container f1-player\e[39m\n\e[94mPod player-pod-6bfc575d89-blgtv:\e[39m \e[33mBackOff - Back-off restarting failed container\e[39m\e[37m\e[39m\n\e[37m\e[39m\n\e[37m━━━ Pod logs ━━━\e[39m\n\e[37m\e[39m\e[90m<Showing last 30 lines per pod in this Deployment. Run the following command for complete logs>\e[39m\n\e[90m$ kubectl -n suitcaselab-default --context=gke_th-structure-flow-demo_us-east1-b_stream-demo logs deployment/player-pod\e[39m\n\e[94m\e[39m\n\e[94m****** player-pod-6bfc575d89-blgtv ******\e[39m\n\e[94m\e[39m\e[90m------ f1-player ------\e[39musage: f1-2020-telemetry-player [-h] [-r REALTIME_FACTOR] [-d DESTINATION]\n                                [-p PORT]\n                                filename\n\nReplay an F1 2019 session as UDP packets.\n\npositional arguments:\n  filename              SQLite3 file to replay packets from\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r REALTIME_FACTOR, --rtf REALTIME_FACTOR\n                        playback real-time factor (higher is faster,\n                        default=1.0)\n  -d DESTINATION, --destination DESTINATION\n                        destination UDP address; omit to use broadcast\n                        (default)\n  -p PORT, --port PORT  destination UDP port (default: 20777)\n"
  resource:
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      name: player-pod
      namespace: suitcaselab-default
      selfLink: /apis/apps/v1/namespaces/suitcaselab-default/deployments/player-pod
      uid: 3bad6370-1b52-42be-bcce-9acbb58088fb
      resourceVersion: '10337958'
      generation: 2
      creationTimestamp: '2021-05-04T18:45:53Z'
      labels:
        app: f1-player-deploy
        service: player
        test: test2
      annotations:
        deployment.kubernetes.io/revision: '2'
        garden.io/hot-reload: 'false'
        garden.io/manifest-hash: 84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"garden.io/hot-reload":"false","garden.io/manifest-hash":"84185df5063487a1be1ed0d0a4de85f5028c234f292f544b927747d30866f921","service":"player"},"labels":{"app":"f1-player-deploy","service":"player","test":"test2"},"name":"player-pod","namespace":"suitcaselab-default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"f1-player"}},"template":{"metadata":{"labels":{"app":"f1-player"}},"spec":{"containers":[{"args":["-h"],"image":"mgunter/python-f1-play","name":"f1-player","ports":[{"containerPort":20777,"name":"udp-server","protocol":"UDP"}],"volumeMounts":[{"mountPath":"/sensable","name":"data-pv-storage"}]}],"restartPolicy":"Always","volumes":[{"name":"data-pv-storage","persistentVolumeClaim":{"claimName":"data-pv-claim"}}]}}}}
        service: player
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: f1-player
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: f1-player
        spec:
          volumes: []
          containers: []
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
    status:
      observedGeneration: 2
      replicas: 2
      updatedReplicas: 1
      unavailableReplicas: 2
      conditions: []


[2021-05-04T20:03:43.472Z] Command "/Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl --context=docker-desktop apply --prune --selector service=flink-session-cluster --output=json -f -" failed with code 1:

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: dial tcp 10.108.93.76:443: connect: connection refused

Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s: dial tcp 10.108.93.76:443: connect: connection refused

Error Details:

shortMessage: >-
  Command failed with exit code 1:
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector
  service=flink-session-cluster --output=json -f -
command: >-
  /Users/mattgunter/.garden/tools/kubectl/fd63e19492c91ef3/kubectl
  --context=docker-desktop apply --prune --selector
  service=flink-session-cluster --output=json -f -
exitCode: 1
stdout: ''
stderr: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  dial tcp 10.108.93.76:443: connect: connection refused
all: >-
  Error from server (InternalError): error when creating "STDIN": Internal error
  occurred: failed calling webhook "mflinkcluster.flinkoperator.k8s.io": Post
  https://flink-operator-webhook-service.flink-operator-system.svc:443/mutate-flinkoperator-k8s-io-v1beta1-flinkcluster?timeout=30s:
  dial tcp 10.108.93.76:443: connect: connection refused
failed: true
timedOut: false
isCanceled: false
killed: false


[2021-05-04T20:03:43.520Z] 1 deploy task(s) failed!

Error Details:

results:
  deploy.flink-session-cluster:
    type: deploy
    description: >-
      deploying service 'flink-session-cluster' (from module
      'flink-session-cluster')
    key: deploy.flink-session-cluster
    name: flink-session-cluster
    startedAt: 2021-05-04T20:03:42.740Z
    completedAt: 2021-05-04T20:03:43.454Z
    batchId: d1db79dc-6125-44d0-a2ec-8c54c70a7cba
    version: v-ee21b741b7

